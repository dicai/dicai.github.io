[{"authors":["admin"],"categories":null,"content":"I am a PhD student at Princeton University in Computer Science, advised by Ryan P. Adams and Barbara Engelhardt.I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group. Previously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.S. in Statistics from the University of Chicago, and an M.A. in Computer Science from Princeton University. My research was supported by a Google PhD Fellowship in Machine Learning.\nResearch interests: I am broadly interested in developing robust and reliable methods for data analysis and understanding their properties. I\u0026rsquo;m particularly interested in probabilistic inference and uncertainty quantification, with a focus on Bayesian methods under model misspecification, approximate inference, active learning, and applications to material science and biomedical data science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dicai.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at Princeton University in Computer Science, advised by Ryan P. Adams and Barbara Engelhardt.I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group. Previously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.S. in Statistics from the University of Chicago, and an M.A. in Computer Science from Princeton University.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":"","date":1684713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"d20737813b6367d70ecbbf56675d80ad","permalink":"https://dicai.github.io/talk/2023flatironsymposium/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/talk/2023flatironsymposium/","section":"talk","summary":"","tags":[],"title":"TBD","type":"talk"},{"authors":[],"categories":null,"content":"","date":1681603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"95a88870a2ec3c92c81c8f191d8fc08f","permalink":"https://dicai.github.io/talk/2023gpseminar/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/talk/2023gpseminar/","section":"talk","summary":"","tags":[],"title":"Multi-fidelity scientific discovery and design","type":"talk"},{"authors":[],"categories":null,"content":"","date":1675728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"7cc3e399d419d1bcb02d09d4ac1ce6e1","permalink":"https://dicai.github.io/talk/2023msrnyc/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/talk/2023msrnyc/","section":"talk","summary":"","tags":[],"title":"Machine learning for scientific discovery: is refining my model worth the cost?","type":"talk"},{"authors":[],"categories":null,"content":"","date":1674432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"9905f75c7fd795d8fe3bce1849492455","permalink":"https://dicai.github.io/talk/2023harvardstats/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/talk/2023harvardstats/","section":"talk","summary":"","tags":[],"title":"Machine learning for scientific discovery: is refining my model worth the cost?","type":"talk"},{"authors":["Aishwarya Mandyam","Didong Li","Diana Cai","Andrew Jones","Barbara E. Engelhardt"],"categories":null,"content":"A preliminary version appeared in the Symposium on Advances in Approximate Bayesian Inference, 2022.\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"68ae55d193816058660d4171ab6ec787","permalink":"https://dicai.github.io/publication/conference-paper/2022-bayesian-irl-kernel-density/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/2022-bayesian-irl-kernel-density/","section":"publication","summary":"Inverse reinforcement learning (IRL) is a powerful framework to extract the reward function of an agent by observing its behavior, but IRL algorithms that infer point estimates can be misleading. A Bayesian approach to IRL can model a distribution over possible reward functions that could explain a set of observations, alleviating the shortcomings of learning a point estimate.  Unfortunately, existing Bayesian approaches to IRL use a $Q$-value function, estimated using $Q$-learning, in place of the likelihood function. The resulting posterior is computationally intensive to calculate, and has few theoretical guarantees. We introduce kernel density Bayesian IRL (KD-BIRL), a method that uses conditional kernel density estimation to directly approximate the likelihood used in Bayesian inference. The resulting posterior distribution contracts to the optimal reward function as the dataset sample size increases, leading to a flexible and efficient framework that extends to environments with complex state spaces. We demonstrate KD-BIRL's computational benefits and ability to represent uncertainty in the recovered reward function through a series of experiments in a Gridworld environment and on a healthcare task.","tags":["Bayesian inverse reinforcement learning"],"title":"Kernel density Bayesian inverse reinforcement learning","type":"publication"},{"authors":["Andrew Jones","Diana Cai","Didong Li","Barbara E. Engelhardt"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"923e284cf6cea5ceaf4a43e7b8ee1cb0","permalink":"https://dicai.github.io/publication/journal-article/2022-spatial-genomics-experimental-design/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal-article/2022-spatial-genomics-experimental-design/","section":"publication","summary":"Spatially-resolved genomic technologies have shown promise for studying the relationship between the structural arrangement of cells and their functional behavior. While numerous sequencing and imaging platforms exist for performing spatial transcriptomics and spatial proteomics profiling, these experiments remain expensive and labor-intensive. Thus, when performing spatial genomics experiments using multiple tissue slices, there is a need to select the tissue cross sections that will be maximally informative for the purposes of the experiment. In this work, we formalize the problem of experimental design for spatial genomics experiments, which we generalize into a problem class that we call _structured batch experimental design_. We propose approaches for optimizing these designs in two types of spatial genomics studies\u0026colon; one in which the goal is to construct a spatially-resolved genomic atlas of a tissue, and another in which the goal is to localize a region of interest in a tissue, such as a tumor. We demonstrate the utility of these optimal designs, where each slice is a two-dimensional plane, on several spatial genomics datasets.","tags":["experimental design"],"title":"Optimizing the design of spatial genomics studies","type":"publication"},{"authors":null,"categories":null,"content":"With researchers at Princeton, Stanford, and UNC, I am collaborating on several application areas related to the biomedical health sciences:\n Experimental design for spatial genomics Bayesian inverse reinforcement learning for health applications Efficient online changepoint detection in time series  ","date":1670630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670630400,"objectID":"11b2923eceeaa6299ada9d19ba161271","permalink":"https://dicai.github.io/project/biomedical/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/project/biomedical/","section":"project","summary":"With researchers at Princeton, Stanford, and UNC, I am collaborating on several application areas related to the biomedical health sciences:\n Experimental design for spatial genomics Bayesian inverse reinforcement learning for health applications Efficient online changepoint detection in time series  ","tags":["applications"],"title":"Machine learning for biomedical health applications","type":"project"},{"authors":null,"categories":null,"content":"Machine learning has tremendous prediction to accelerate the design of novel materials. I currently collaborate with researchers at Princeton, the Colorado School of Mines, UIUC, and WashU on applications of machine learning and probabilistic methods to material science problems. We are developing new methods for active search, Markov Chain Monte Carlo simulation, and combinatorial design. Current application areas include machine learning for alloy design, ML-assisted property prediction, and topology optimization of structural materials.\n","date":1670630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670630400,"objectID":"37d64c614a5d0f71940d105625fe2c0a","permalink":"https://dicai.github.io/project/materials/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/project/materials/","section":"project","summary":"Machine learning has tremendous prediction to accelerate the design of novel materials. I currently collaborate with researchers at Princeton, the Colorado School of Mines, UIUC, and WashU on applications of machine learning and probabilistic methods to material science problems. We are developing new methods for active search, Markov Chain Monte Carlo simulation, and combinatorial design. Current application areas include machine learning for alloy design, ML-assisted property prediction, and topology optimization of structural materials.","tags":["applications"],"title":"Machine learning for material science","type":"project"},{"authors":["Diana Cai","Ryan P. Adams"],"categories":null,"content":"Citation @inproceedings{cai2022multi, title={Multi-fidelity Monte Carlo: a pseudo-marginal approach}, author={Diana Cai and Ryan P Adams}, booktitle={Advances in Neural Information Processing Systems 35}, year={2022} } ","date":1670198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670198400,"objectID":"3114260c438ecc466a7686fabbdb528d","permalink":"https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-mcmc/","publishdate":"2022-12-05T00:00:00Z","relpermalink":"/publication/conference-paper/2022-multi-fidelity-mcmc/","section":"publication","summary":"Markov chain Monte Carlo (MCMC) is an established approach for uncertainty quantification and propagation in scientific applications.  A key challenge in applying MCMC to scientific domains is computation -- the target density of interest is often a function of expensive computations, such as a high-fidelity physical simulation, an intractable integral, or a slowly-converging iterative algorithm.  Thus, using an MCMC algorithms with an expensive target density becomes impractical, as these expensive computations need  to be evaluated at each iteration of the algorithm.  In practice, these computations often approximated via a cheaper, low-fidelity computation, leading to bias in the resulting target density.  Multi-fidelity MCMC algorithms combine models of varying fidelities in order to obtain an approximate target density with lower computational cost.  In this paper, we describe a class of asymptotically exact multi-fidelity MCMC algorithms for the setting where a sequence of models of increasing fidelity can be computed that approximates the expensive target density of interest.  We take a pseudo-marginal MCMC approach for multi-fidelity inference that utilizes a cheaper, randomized-fidelity unbiased estimator of the target fidelity constructed via  random truncation of a telescoping series of the low-fidelity sequence of models.  Finally, we discuss and evaluate the proposed multi-fidelity MCMC approach on several applications, including log-Gaussian Cox process modeling, Bayesian ODE system identification, PDE-constrained optimization, and Gaussian process regression parameter inference.","tags":["slice sampling","multi-fidelity modeling","Bayesian inference"],"title":"Multi-fidelity Monte Carlo: a pseudo-marginal approach","type":"publication"},{"authors":["Andrew Jones*","Diana Cai*","Barbara E. Engelhardt"],"categories":null,"content":"","date":1667347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667347200,"objectID":"43c29f219ef4d9ab3d480812a32a4686","permalink":"https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-experimental-design/","publishdate":"2022-11-02T00:00:00Z","relpermalink":"/publication/conference-paper/2022-multi-fidelity-experimental-design/","section":"publication","summary":"As experimental tools in the physical and life sciences become increasingly sophisticated and costly, there is a need to optimize the choice of experimental parameters to maximize the informativeness of the data and minimize cost. When designing a scientific experiment, an experimentalist often faces a choice among a suite of data collection modalities or instruments with varying fidelities and costs. Analyzing the tradeoff between high-fidelity, high-cost measurements and low-fidelity, low-cost measurements is often difficult due to complex data collection procedures and budget constraints. Here, we propose an approach for designing such experiments using Bayesian power posteriors, which naturally account for instruments with varying fidelities. Whereas existing approaches for multi-fidelity experimental design are often bespoke for particular data models and involve complicated inference schemes, our approach using power posteriors is generically applicable for any probabilistic model and straightforward to implement. We show that our approach can be combined with a model of experiment cost to allow for end-to-end multi-fidelity experimental design. We demonstrate our approach through a series of simulated examples and an application to a genomics experiment.","tags":["multi-fidelity methods"],"title":"Multi-fidelity Bayesian experimental design using power posteriors","type":"publication"},{"authors":[],"categories":null,"content":"","date":1653350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648598400,"objectID":"217a459bfb06c4cacbffeb9b404da8f8","permalink":"https://dicai.github.io/talk/2022ness/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/talk/2022ness/","section":"talk","summary":"","tags":[],"title":"Multi-fidelity Monte Carlo: a pseudo-marginal approach","type":"talk"},{"authors":[],"categories":null,"content":"","date":1643760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"869d576dfadfa9279ae1a2884943936a","permalink":"https://dicai.github.io/talk/2022csp/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2022csp/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":["Aishwarya Mandyam","Didong Li","Diana Cai","Andrew Jones","Barbara E. Engelhardt"],"categories":null,"content":"","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"06743e4b8fe758be2c53701c780834d0","permalink":"https://dicai.github.io/publication/conference-paper/2022-birl-aabi/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/publication/conference-paper/2022-birl-aabi/","section":"publication","summary":"Inverse reinforcement learning (IRL) methods attempt to recover the reward function of an agent by observing its behavior. Given the large amount of uncertainty in the underlying reward function, it is often useful to model this function probabilistically, rather than estimate a single reward function.  However, existing Bayesian approaches to IRL use a $Q$-value function to approximate the likelihood, leading to a computationally intractable and inflexible framework. Here, we introduce kernel density Bayesian IRL (KD-BIRL), a method that uses kernel density estimation to approximate the likelihood. This lends itself to an efficient posterior inference for the reward function given a sequence of agent observations. Using both linear and nonlinear reward functions in a Gridworld environment, we demonstrate that the KD-BIRL posterior centers around the true reward function, and that our method is more efficient than existing approaches.","tags":["Bayesian inverse reinforcement learning"],"title":"Efficient Bayesian inverse reinforcement learning via conditional kernel density estimation","type":"publication"},{"authors":[],"categories":null,"content":"","date":1635897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"6b45e6c6106cb5592276ac274a105340","permalink":"https://dicai.github.io/talk/2021birs/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2021birs/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":[],"categories":null,"content":"","date":1633132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"c42b9e465ec5c020f60dde06910de049","permalink":"https://dicai.github.io/talk/2021ness/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2021ness/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":["David M. Zoltowski","Diana Cai","Ryan P. Adams"],"categories":null,"content":"Preliminary version appeared in the AABI Symposium in January 2021.\nCitation @inproceedings{zoltowski2021slice, title={Slice Sampling Reparameterization Gradients}, author={David M. Zoltowski and Diana Cai and Ryan P Adams}, booktitle={Advances in Neural Information Processing Systems 34}, year={2021} } ","date":1632787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632787200,"objectID":"b8c9bc812d29224197f0db1fd8f3fc49","permalink":"https://dicai.github.io/publication/conference-paper/2021-reparameterized-gradients-slice-sampling/","publishdate":"2021-09-28T00:00:00Z","relpermalink":"/publication/conference-paper/2021-reparameterized-gradients-slice-sampling/","section":"publication","summary":"Many probabilistic modeling problems in machine learning use gradient-based optimization in which the objective takes the form of an expectation. These problems can be challenging when the parameters to be optimized determine the probability distribution under which the expectation is being taken, as the naive Monte Carlo procedure is not differentiable. Reparameterization gradients make it possible to efficiently perform optimization of these Monte Carlo objectives by transforming the expectation to be differentiable, but the approach is typically limited to distributions with simple forms and tractable normalization constants. Here we describe how to differentiate samples from slice sampling to compute _slice sampling reparameterization gradients_, enabling a richer class of Monte Carlo objective functions to be optimized. Slice sampling is a Markov chain Monte Carlo algorithm for simulating samples from probability distributions; it only requires a density function that can be evaluated point-wise up to a normalization constant, making it applicable to a variety of inference problems and unnormalized models. Our approach is based on the observation that when the slice endpoints are known, the sampling path is a deterministic and differentiable function of the pseudo-random variables, since the algorithm is rejection-free. We evaluate the method on synthetic examples and apply it to a variety of applications with reparameterization of unnormalized probability distributions.","tags":["stochastic gradients","slice sampling","MCMC"],"title":"Slice sampling reparameterization gradients","type":"publication"},{"authors":[],"categories":null,"content":"","date":1631664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"3e313d9486b9c33faf483e8a499a07be","permalink":"https://dicai.github.io/talk/2021aalto/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2021aalto/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":[],"categories":null,"content":"","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"9368d314ffdd6af5d2ce2b76759e83b9","permalink":"https://dicai.github.io/talk/2021isba/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2021isba/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":["Diana Cai*","Trevor Campbell*","Tamara Broderick"],"categories":null,"content":"Preliminary versions appeared in NeurIPS 2019 Workshop on Machine Learning with Guarantees and also the Symposium on Advances in Approximate Bayesian Inference 2017, co-located with NeurIPS 2017.\nCitation @InProceedings{pmlr-v139-cai21a, title = {Finite mixture models do not reliably learn the number of components}, author = {Cai, Diana and Campbell, Trevor and Broderick, Tamara}, booktitle = {Proceedings of the 38th International Conference on Machine Learning}, pages = {1158--1169}, year = {2021}, volume = {139}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR} } ","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"bde318b604697596bb314a9805176074","permalink":"https://dicai.github.io/publication/conference-paper/2021-finite-mixtures-unreliable/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/2021-finite-mixtures-unreliable/","section":"publication","summary":"Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. A common suggestion is to use a finite mixture model (FMM) with a prior on the number of components.  Past work has shown the resulting FMM component-count posterior is consistent; that is, the posterior concentrates on the true, generating number of components.  But consistency requires the assumption that the component likelihoods are perfectly specified, which is unrealistic in practice.  In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM component-count posterior _diverges_\u0026#x3a; the posterior probability of any particular finite number of components converges to 0 in the limit of infinite data.  Contrary to intuition, posterior-density consistency is not sufficient to establish this result.  We develop novel sufficient conditions that are more realistic and easily checkable than those common in the asymptotics literature. We illustrate practical consequences of our theory on simulated and real data.","tags":["mixture models","learning the number of components","model misspecification"],"title":"Finite mixture models do not reliably learn the number of components","type":"publication"},{"authors":["Gregory G. Gundersen","Diana Cai","Chuteng Zhou","Barbara E. Engelhardt","Ryan P. Adams"],"categories":null,"content":"Citation @article{gundersen2021active, title={Active multi-fidelity Bayesian online changepoint detection}, author={Gundersen, Gregory W and Cai, Diana and Zhou, Chuteng and Engelhardt, Barbara E and Adams, Ryan P}, journal={arXiv preprint arXiv:2103.14224}, year={2021} } ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"7fc8012116e5111289c420b7b5c4ed3c","permalink":"https://dicai.github.io/publication/conference-paper/2021-active-multi-fidelity-bayesian-changepoint-detection/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/conference-paper/2021-active-multi-fidelity-bayesian-changepoint-detection/","section":"publication","summary":"Online algorithms for detecting changepoints, or abrupt shifts in the behavior of a time series, are often deployed with limited resources, e.g., to edge computing settings such as mobile phones or industrial sensors. In these scenarios it may be beneficial to trade the cost of collecting an environmental measurement against the quality or \"fidelity\" of this measurement and how the measurement affects estimation of a changepoint. For instance, one might decide between inertial measurements or GPS to determine changepoints for motion. A Bayesian approach to changepoint detection is particularly appealing because we can represent our posterior uncertainty about changepoints and make active, cost-sensitive decisions about data fidelity to reduce this posterior uncertainty. Moreover, the total cost could be dramatically lowered through active fidelity switching, while remaining robust to changes in data distribution. We propose a multi-fidelity approach that makes cost-sensitive decisions about which data fidelity to collect based on maximizing information gain with respect to changepoints. We evaluate this framework on synthetic, video, and audio data and show that this information-based approach results in accurate predictions while reducing total cost.","tags":["Bayesian inference","changepoint detection","time series","multi-fidelity modeling"],"title":"Active multi-fidelity Bayesian online changepoint detection","type":"publication"},{"authors":["David M. Zoltowski","Diana Cai","Ryan P. Adams"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"52d53c70ec3bc94d556fb7d1cb62e66e","permalink":"https://dicai.github.io/publication/conference-paper/2021-reparam-slice-wkshp/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/2021-reparam-slice-wkshp/","section":"publication","summary":"Slice sampling is a Markov chain Monte Carlo algorithm for simulating samples from probability distributions, with the convenient property that it is rejection-free. When the slice endpoints are known, the sampling path is a deterministic function of noise variables since there are no accept-reject steps like those in Metropolis-Hastings algorithms. Here we describe how to differentiate the slice sampling path to compute slice sampling reparameterization gradients. Since slice sampling does not require a normalizing constant, this allows for computing reparameterization gradients of samples from potentially complicated multivariate distributions.  We apply the method in synthetic examples and to fit a variational autoencoder with a conditional energy-based model approximate posterior.","tags":["stochastic gradients","slice sampling","MCMC"],"title":"Slice sampling reparameterization gradients","type":"publication"},{"authors":[],"categories":null,"content":"","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"7679c9f47fa56f1e527721f892cc91fc","permalink":"https://dicai.github.io/talk/2020baysmo/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2020baysmo/","section":"talk","summary":"","tags":[],"title":"Finite mixture models do not reliably learn the number of components","type":"talk"},{"authors":[],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"897224e3d55f8eb504753e89c4dbe407","permalink":"https://dicai.github.io/talk/2020umd/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/talk/2020umd/","section":"talk","summary":"","tags":[],"title":"Probabilistic inference under model misspecification","type":"talk"},{"authors":["Diana Cai*","Trevor Campbell*","Tamara Broderick"],"categories":null,"content":"See also: Finite mixture models do not reliably learn the number of components (arXiv e-print 2007.04470) for results on learning the number of components in a finite mixture model via the usual posterior distribution.\n","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"5c234487e82ef7359b2f76b042d8d3ef","permalink":"https://dicai.github.io/publication/conference-paper/2020-power-mix/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/publication/conference-paper/2020-power-mix/","section":"publication","summary":"Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. Data science folk wisdom tells us that a finite mixture model (FMM) with a prior on the number of components will fail to recover the true, data-generating number of components under model misspecification.  But practitioners still widely use FMMs to learn the number of components, and statistical machine learning papers can be found recommending such an approach.  Increasingly, though, data science papers suggest potential alternatives beyond vanilla FMMs, such as power posteriors, coarsening, and related methods.  In this work we start by adding rigor to folk wisdom and proving that, under even the slightest model misspecification, the FMM component-count posterior diverges\u0026#x3a; the posterior probability of any particular finite number of latent components converges to 0 in the limit of infinite data.  We use the same theoretical techniques to show that power posteriors with fixed power face the same undesirable divergence, and we provide a proof for the case where the power converges to a non-zero constant.  We illustrate the practical consequences of our theory on simulated and real data.  We conjecture how our methods may be applied to lend insight into other component-count robustification techniques.","tags":["robustness","mixture models","model misspecification","learning the number of components"],"title":"Power posteriors do not reliably learn the number of components in a finite mixture","type":"publication"},{"authors":null,"categories":null,"content":"Starting a PhD in itself is full of many challenges, but students starting this fall will have to navigate the extra hurdle of a virtual environment. Here I’m attempting to write down what I’d tell my junior colleagues starting a PhD in the US \u0026ndash; geared toward students in Computer Science, and especially Machine Learning \u0026ndash; including some Princeton-specific thoughts.\nNavigating the academic landscape In graduate school, you get an immense amount of freedom to choose how to spend your time. Figuring out how to weigh and adjust your priorities is a skill that will be developed over time.\nGetting started with research At Princeton, in your first year or so, you generally are free to explore working with different professors before committing to an official advisor (or co-advisors) \u0026ndash; take advantage of this early on, but also throughout your PhD.\nUsually your advisor will help you get started with a new research project and suggest a few directions to get started with; maybe your advisor will want you to come in with a few ideas of your own and help you shape those ideas. Try to schedule regular meetings if possible, especially if you find this helpful or motivating (this is usually an aspect of “advisor style” to keep in mind).\nIn practice, advisors are fairly busy and generally expect a lot of independence from you as a student (but you should ask your advisor directly what their expectations are!). To supplement whatever guidance you receive from your advisor(s) on a research project, I’d also suggest:\n Try to join a project with a senior PhD student (or postdoc) that has a few publications already: one way to develop strong research skills is by working with someone who has already published a few impactful papers. This will generally help you get up to speed on, e.g., what background material to learn, how to run experiments on the cluster, how to write a paper, what are the typical venues to submit to, what are some future extensions, how to choose research projects, how to write a paper rebuttal, how to submit to the arXiv, etc. Get to know other professors in your department/area: these may be eventual committee members, co-advisors, or collaborators. How do you get to know other professors? Ask questions in class and attend office hours, take seminars from professors, ask faculty members if you can sit in their group meetings or reading groups. Get advice on what papers, books, and resources to check out from other students and postdocs. But also try to avoid groupthink and figure out what problems excite you, rather than gravitating to hot areas by default. Attend relevant seminars and talks to get an idea of what types of research people are currently thinking about. Jot down the main takeaway of the talk, and make a mental note if there is a paper or preprint online.  I’d also suggest keeping a (digital) notebook of research ideas and adding to it every time inspiration strikes. Many of these will probably be “bad” ideas, but sometimes even bad ideas can get shaped into solid research contributions.\nLastly, I think it\u0026rsquo;s important to focus on building research skills, rather than worrying too much about working on ambitious research projects. Your PhD research will help you contribute new knowledge to some area of research, but does not define what you work on for the rest of your career!\nCheck out:\n Matt Might\u0026rsquo;s Illustrated Guide to a PhD. Nick Feamster - Do You Need a Ph.D.?  Balancing classes (breadth \u0026amp; foundations) vs research (depth) Over time, I’ve come to see coursework as an opportunity to learn from someone who is an expert in some area and as a chance to really expand your toolkit. So I’d recommend trying to take some foundational courses in your earlier years if you can, especially if you haven’t had much previous exposure to particular topics, as well as some topics courses to help expand your breadth.\nOn the virtual format: Zoom lectures can be tough, but it\u0026rsquo;s not a bad idea to spend some more time on courses in your first year. Courses add some much-needed extra structure during a virtual term and allow you to meet other students that you might not ordinarily interact with.\nAt Princeton, some excellent foundational courses I’d recommend for ML students (in CS, ORFE, and EE) include:\n Theoretical Machine Learning Probability Theory Probability in High-Dimension (and/or Mathematics of Data Science) Statistical Inference (and/or Nonparametric Statistics) Convex Optimization (and/or Nonlinear Optimization and/or Computational Statistics)  More generally, talk to other students to find out what courses they enjoyed and found useful (or if there is an instructor that really stands out).\nOn seminars: I have some ambivalence about taking research seminars, especially paper reading and presentation courses (I have taken many such courses). But I think the right ones can be a really great way to learn about a narrow topic relevant to your research and to get to know a professor in a specific research area you’re interested in.\nFor example, at Princeton I really enjoyed taking a seminar on Optimization for Machine Learning. I liked this because it was a lecture-based course covering both foundational topics and modern research progress. The course had a few simple assignments to help reinforce my understanding of the material but were also not too time consuming.\nMy suggestion is to only take a few seminars, and to wait until later in your PhD when you have less time for more intensive coursework but still want to gain breadth in some new areas. If there is a topic that you’re really interested in or is your specific research area and you see a seminar being taught, then of course I’d recommend checking it out, since these aren’t usually taught every year!\nThe CS program at Princeton gives you a lot of flexibility of when you can take courses and which courses to take. Thus, this give you some choice in how to balance your course schedule with research. For my required breadth areas, I took Advanced Algorithms and Networks. During my first 3 years, I chose to take one semester that was more course-focused (e.g., one or more courses with weekly or bi-weekly assignments) and one semester that was more research-focused (e.g., at most 1 research seminar course), and I’d typically shoot for some type of paper submission deadline during the term when I wasn’t doing much coursework (RIP Spring 2020).\nBuilding communication skills Communication skills are crucial to develop for researchers. We often hear about communication skills in giving talks and writing papers (which is very important!). Early on in your graduate career, you’ll also want to think about how to have effective meetings \u0026ndash; this is especially important given the virtual nature of meetings right now.\nLeading effective research meetings Maybe you\u0026rsquo;ve gotten some awesome results for a project, or maybe you\u0026rsquo;re stuck on something and are meeting with your advisor and/or collaborators. Beyond just preparing the technical content of what you’re planning to discuss, think about how to communicate your results in a clear and efficient manner so as to minimize your collaborators' cognitive load and ultimately get the best feedback possible.\nFor in-person (or video call) meetings, you may want to experiment with coming in with an agenda of items you want to discuss (or even consider sending this out in advance). If you’re presenting some results, remind your collaborators of the setting and your last conversation(s) before sharing results or updates; e.g., don’t just screen share and jump into page 4 of your notes \u0026ndash; briefly remind everyone of what\u0026rsquo;s going on.\nIf you are collaborating with someone through, e.g., email or Slack, you\u0026rsquo;ll want to provide a little bit more context of what you\u0026rsquo;re trying to communicate. For example, don’t just send them a figure with little to no context, and don’t assume they remember the details of your last conversation. At the minimum a concise writeup of the setting is useful, for instance:\n What data are you using (if synthetic - how was it generated?) What model are you using and how was it fit? Hyperparameters? What is a summary of the conclusions/takeaways you want to convey? What questions do you have? What are you stuck on? What are you thinking about doing next?  Read over your writeup and double check that it clearly and precisely communicates the points you\u0026rsquo;re trying to convey. Make sure any plots you include are clearly labeled. Keep things short to maximize the chance it\u0026rsquo;ll actually get read.\nKeep detailed notes of all of your meetings \u0026ndash; this is especially important if you start working on multiple projects and need to context switch. At the end of a meeting, sometimes it’s helpful to verbally discuss concretely what your TODOs are for next time. This may help you focus on what to do next and also help get some direction from your advisor/collaborators.\n1-1 meetings with your advisor Here\u0026rsquo;s a great zine from Julia Evans \u0026ndash; it\u0026rsquo;s geared towards industry, but I think the main ideas apply in terms of meetings with your advisor. Besides the technical specifics of a research project, you should discuss other things like:\n What should I read to learn about [topic]? I\u0026rsquo;d like to brainstorm about [potential research direction]. What should I expect for my [generals exam, defense, conference talk]? I want apply for [fellowship, award, internship, contributed talk]. How do I do [write a research statement, give a talk, write a paper]? I think we could improve [the reading group] by doing [suggestion]. Do you have any feedback on [the talk I gave, my research statement]? What are the typical job trajectories after the PhD?  Writing and presentation skills Writing and presentation skills are often under-emphasized in graduate school training but are so crucial for getting paper acceptances, communicating your ideas, and getting funding for new research.\nIn my experience, the best way to get better at giving talks and writing papers is to get a lot of practice in. Keeping a research blog (or detailed notes) can help you practice how to clearly articulate a technical topic. Besides advisors and collaborators, you can also get feedback on paper drafts from peers \u0026ndash; this feedback gives you perspective on potential comments you may get from reviewers or readers that you can work on clarifying.\nPractice giving talks and presentations as much as possible, and get feedback on what could be more clear \u0026ndash; group meetings, reading groups, and class presentations are a great way to get started with this. When you watch a talk you really like, make a note of what was great about it. If you see a talk you thought could have been better, make a note of why. Later on, you\u0026rsquo;ll give talks at conferences and workshops and at other universities, and it\u0026rsquo;s great to be extra prepared before you start speaking in front of larger audiences.\nPractice your talks before you give them! This will help you with your general delivery and flow, even when you get nervous. Before you give a generals or qualifying exam talk, schedule a practice talk with your peers and/or ask your advisor if you can give a practice talk. You can also practice and record your talks on Zoom \u0026ndash; this allows you to watch the talk back and see what you can improve on.\nAdditional reading:\n Margo Seltzer\u0026rsquo;s Tips on Writing on Thesis Jacob Steinhardt\u0026rsquo;s Advice for Authors Lipton and Steinhardt - Trouble Trends in Machine Learning Scholarship Trevor Campbell on \u0026ldquo;How to Explain Things\u0026rdquo;  Navigating the literature As a graduate student, you’ll quickly realize how many papers there are: journal papers, conference papers, workshop papers, arXiv papers, etc. You might wonder how to possibly keep up with all the papers being put out every day. My answer to this is: you don’t \u0026ndash; but more on this later.\nSome ways to read papers include:\n High-level browsing: reading the title, authors, and (maybe) abstract Big-picture read: high-level browsing + reading the introduction and skimming the sections of the paper. Detailed read: big-picture read + carefully reading (and re-reading) many or all of the technical sections of the paper  My workflow: Most papers I read only get a big-picture read. I also do a lot of high-level browsing of papers from daily arXiv email blasts (I’m subscribed to stat.ML and math.ST), and I generally make a note of anything directly relevant to my research. If it’s not too relevant to my work but seems interesting, I try to save it using the Google Keep browser plugin.\nTypically papers I read carefully are longer, journal articles and take a lot of time for me to digest; these are usually papers directly related to a research project I’m currently working on or papers that provide a deep understanding of a new topic I\u0026rsquo;m trying to understand. Also, if there\u0026rsquo;s a talk available (e.g., from a conference, workshop, tutorial, or summer school), I try to watch that before reading the paper in depth; that way I come in already knowing the main idea and concepts before diving into the details.\nTips for early on: keep a detailed note-tracking system for papers, and have these backed up via cloud storage or version control. You may even consider keeping a citation system. When you do a big-picture read, it’s helpful to write down your version of the big picture summary, so you can refer back to it later. This is especially useful when it’s time to write up a paper and you’re going back to review and organize related work. Taking notes and revisiting them upon second or third readings of a paper is also useful for gaining new insights on a topic \u0026ndash; oftentimes when you’re new to a topic, you don’t have as much context or background to fully understand some parts of a paper.\nI am generally skeptical of tools that you have to pay a monthly subscription to use, as I value persistence of my notes and workflow (and sometimes tools stop being supported or start charging for more than two devices). Here are some tools I personally use as of the current writing:\n For detailed research notes: I like to have these notes in LaTeX + pdf format and in backed up in a git repository. Notability for iOS and MacOS: I’ve been using my ipad to keep a lot of notes on each of my projects, meeting notes, notes from talks and lectures, and paper annotation notes. Recently I also got the desktop app in order to make it easier to add papers and typed notes. Google Keep for super quick notes: e.g., I save papers that might be relevant after doing a high-level browse using the browser extension. I keep detailed folders of papers (in Dropbox) to allow for fast search of titles/authors. Using Jekyll/Hugo (or similar tool) + MathJax + git to keep LaTeX notes in a blog format: this makes it nice to organize notes by topic (or give some notes multiple topics) and to in general customize your note-taking format. Note: you can just organize this locally without posting your notes online. Google docs: for collaborative projects, sometimes I keep lists of related work with notes in shared google docs / folders. Paper notebooks: I do like to write on paper, but I try to make sure to transfer anything important to LaTeX or some other digital notebook so that I don’t lose it.  Find a format that works for you so that you can refer back to them in 5 years, and keep those notes under version control or backed up in some way.\nAside: when I first started doing ML research, I kept most of my notes in a paper notebook. Now I love some good Muji products \u0026ndash; but I found that I rarely refer back to handwritten notes and eventually lose track of physical pages and notebooks. Since switching over to more digital notetaking tools, I\u0026rsquo;ve definitely noticed a difference in my ability to keep track of multiple projects and ideas.\nBuilding strong relationships Finding a good set of collaborators is often key and can make research more fun. Learning from a diverse set of people often helps you develop new perspectives and leads to more creative thinking. Finding good collaborators can take time (and is often more difficult for students from underrepresented groups), but one way to get started is to first build your research network.\nVirtual coffee chats Given the virtual environment, building community will be even harder than before, so it’s important to stay proactive. I’d highly recommend trying to schedule a weekly 1-1 coffee chat with other students and postdocs in your lab and in your department. This could lead to friendships and collaborations but also just keep you engaged in the community.\n1-1 meetings with visitors Usually your advisor will also have visiting researchers in the lab and have students sign up to meet with these researchers. This is another great opportunity to get to know another researcher and to share your own research. If you’re not sure what to talk about, some general topics include:\n General introductions about your backgrounds Ask about what they’re currently excited about Tell them a bit about your current interests or a project you’re working on  Generally, researchers and academics love to talk about their own work, and so if you feel like you don’t have much to say yourself, this is an easy way to fill a meeting. But I’d also encourage you to chat about some of your interests \u0026ndash; part of the graduate school experience is learning how to get out of your comfort zone and building up your confidence; it’s totally fine to say you just got started with research but are really interested in X because of Y.\nHave a website and update it regularly Graduate students (and yes, first years too) should make a research website. You can usually host on a university domain or on Github pages. If your department has a student directory, they may ask to link to a personal website \u0026ndash; definitely take advantage of this! Similarly, advisors often link to their current students’ websites.\nHaving a website earlier on in your graduate career is an important step toward having some visibility. When you’re applying for jobs (or internships, fellowships, travel grants, etc.), it is helpful to have a website. It’s also very common to look up someone’s website when considering speaker invitations for events or reviewer invitations for academic conferences and workshops. It’s generally considered a professional must-have now, just like how you need to maintain a CV or resume.\nMany students are hesitant about making a website, often due to feeling like they don\u0026rsquo;t have anything to put online. But even if you feel this way, you can still start out with a fairly minimal website: e.g., name, email, a brief bio (what program are you in, where were you before that), and possibly even a few sentences on what you’re interested in now.\nIf you’re comfortable, here are some ideas on other things you might pick and choose from when deciding what to include:\n A professional photo A brief bio: including who you\u0026rsquo;re advised by, if you\u0026rsquo;re part of a larger research group Links to: Github, Google Scholar, Twitter, CV, etc. Past work and teaching experience Research portfolio: pre-prints and working papers, publications, the report for a course project you’re proud of, a description and link of any software packages you’re written Non-academic work you’re proud of (e.g., volunteer work)  Virtual conferences and workshops Attending conferences is a good way to meet people and learn about new research. One of the advantages of the virtual format is that student registration fees are generally fairly low (~$25) or even free, and so your advisor will likely be more willing to support you, even if you\u0026rsquo;re not presenting any work. You can also apply for a registration fee scholarship \u0026ndash; often conferences offer applications for students as well as researchers from underrepresented groups. In machine learning, several of the major conferences have some presence from some affinity group, such as Black in AI, LatinX in AI, {Dis}Ability in AI, and Women in Machine Learning (also other areas such as NLP, CV), and many of these also offer scholarships to register for conferences, such as NeurIPS and ICML.\nRegistering for a virtual conference gives you access to a bunch of recorded talks and panels, as well as a chance to interact with other conference attendees. I’d suggest trying to attend a virtual conference this year, regardless of whether you\u0026rsquo;re presenting anything.\nWhat are the main conference venues in machine learning? Just to list a few:\n NeurIPS ICML AISTATS COLT UAI ICLR AAAI KDD  Some of these are more geared towards certain areas, such as theory or applications. If you’re working at the intersection of ML and statistics, attending statistics conferences (which generally don’t have published proceedings) may also be of interest, e.g.: JSM, ISBA, or smaller workshops in specific topics.\nA great way to participate in conferences early on is to submit to workshops. Typically, many of the larger conferences (e.g., NeurIPS and ICML) offer a few days of workshops that are typically solicit extended abstracts and short papers (e.g., around 2\u0026ndash;4 pages) that are not published in the conference proceedings. This is a great way to meet other people working in this subfield and to get feedback on your work. Typically, the acceptance rate for workshops is much higher than for a conference (but you should still strive to only submit high-quality work!).\nAdditionally, many research institutes often have semester-long or year-long programs with workshops, such as the Simon’s Institute or the Institute of Advanced Study.\nMaintaining and contributing to a healthy work environment Based on my own personal experience and conversations with peers, there are many challenging aspects of grad school (which I’d say is an understatement!), and these will be different for everyone!\nThe main thing I want to emphasize is that it’s important to maintain and contribute to a healthy work environment, and this will probably take some experimenting to figure out what works for you.\nWFH productivity and mental health It’s a huge privilege for computer scientists to be able to work from home on their computers. That said, I’ve definitely found it hard to be productive working from home full-time in the middle of a pandemic. I think it\u0026rsquo;s important to be especially gentle with ourselves and others right now \u0026ndash; personally, I\u0026rsquo;ve been trying to take things one week at a time.\nPhysical and mental health Take care of your physical and mental health \u0026ndash; this is not easy and takes active work! Graduate school itself is filled with moments of imposter syndrome, failure, isolation, and stress. So being proactive about building up your physical and mental health is crucial for preparing you for potential tough situations:\n Get lots of sleep, drink lots of water, and eat healthily. Try to prioritize getting some exercise and spending some time outdoors every day. Establish a strong support group \u0026ndash; talk to a therapist, your mentors, and friends and family regularly. (At Princeton: check out the CS Grad and GSG Slack Groups.) Develop and maintain some hobbies outside of work. Try to maintain a regular sleep schedule and daily routine.  Work from home tips Try to get a good work setup at home, if possible. You might want to consider asking if your advisor, department, or university has resources in helping you get set up to work from home, with e.g., a laptop, monitor, desk, and chair. I’d also check with your advisor (or department) to see if they have funds for a tablet for online meetings \u0026ndash; if not, I’ve found that paper+pen and webcam can also be useful for communicating in online video calls. Try to maintain some separation of work and life, and try to keep your work area clean!\nStay organized \u0026ndash; right now the extra lack of structure and interactions makes it easy for the days to just blend together. These days I really rely on calendar invites to keep track of what I need to do. I\u0026rsquo;ve tried a lot of various organization tools. Currently, I\u0026rsquo;ve been using a super simple system \u0026ndash; I maintain a note with checkboxes of all the tasks I need to do this semester and when I need to do them by. Then I make sure to check it every day to remind myself what needs to get done soon and update the list when new tasks come up. (Update: I\u0026rsquo;ve been using a similar workflow as the one described, but now using Todoist to keep track of long-termer goals and deadlines as well as daily tasks and meetings with the calendar integration.)\nSeveral times I\u0026rsquo;ve looked back at a semester and wondered, \u0026ldquo;What did I do the past few months?\u0026rdquo; This past year, I\u0026rsquo;ve been keeping a daily research log in a Google doc, where I write down all the tasks I did that day (research, meetings, exercise, errands). At the start of each day, I write down tasks I plan to do and update my list of completed tasks as the day goes on. The reason I do this is because it helps me feel like I am moving forward, even if at an extremely slow rate!\nTake a break! Some days are not going to be that productive \u0026ndash; and that’s ok! Take time off if you need to, but also when you don’t need it so you don’t burn out. In academia, there is a culture of overworking and projecting the image of working all the time, and this is something I actively try to resist. During the semester, I almost always take fall and spring break off and take at least a week off during summer/winter holidays. I also try to not work on the weekends and evenings and generally push back now when people try to schedule work meetings on the weekends or during the evening*.\n*Except perhaps around conference deadlines. :-)\nContributing to an inclusive work environment Machine learning (CS / STEM) has a diversity problem. But as graduate students, we each can strive toward improving the inclusivity of our local community, such as our research groups, departments, university, and the greater research community, and to educate ourselves about systemic issues faced by students and researchers from underrepresented groups. For students starting just now, it won’t be long before you become one of the senior members of your research lab and other students will look to you for guidance.\nI’ll close with some recommended reading:\n Combating Anti-Blackness in the AI Community (Devin Guillory) Diversity \u0026amp; Inclusion Resources from Queer in AI CRA Taulbee Survey [PDF Report link] Creating a Positive Lab Culture (Margo Seltzer) NeurIPS 2018 Diversity \u0026amp; Inclusion Survey (Hal Daume III and Katherine Heller) CRA-W Best Practices: Guidelines for running an inclusive conference Gender bias calculator  ","date":1598745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"91bb1a5d0475fd87c1232b45e26a21b0","permalink":"https://dicai.github.io/post/tips-new-phd-students/","publishdate":"2020-08-30T00:00:00Z","relpermalink":"/post/tips-new-phd-students/","section":"post","summary":"Starting a PhD in itself is full of many challenges, but many students starting this fall will have to navigate the extra hurdle of being in a virtual environment along with many other challenges. Here I’m attempting to write down what I’d tell my junior colleagues starting a PhD -- geared toward students in Computer Science, and especially Machine Learning -- including some Princeton-specific thoughts.","tags":null,"title":"Tips for New PhD Students in Machine Learning","type":"post"},{"authors":[],"categories":null,"content":"Fellowships, Scholarships, and Grants Below is a list of fellowships, scholarships, and grants for graduate students, typically geared toward women or underrepesented groups in computing:\n Microsoft Ada Lovelace PhD Fellowship: 2nd year PhD students at time of application Microsoft Dissertation Grant: 4th year PhD or above at time of application Women Techmakers Scholarship: undergraduate, masters, and PhD students Gertrude M. Cox Scholarship: MS or PhD students Adobe Research Women-in-Technology Scholarship: undergraduates and masters students Loreal USA for Women in Science: postdoctoral fellowship  Conference support Many organizations offer travel awards for graduate students and postdocs to attend conferences. Now that many conferences are virtual, conference support is more commonly taking the form of registration support.\nMachine Learning conferences  Google Conference and Travel Scholarships: registration and travel support to attend select conferences NeurIPS and ICML often offer D\u0026amp;I scholarships for underrepresented groups in ML, including women ML/AI affinity groups: often offer travel (or registration) assistance to co-located conferences  Women in Machine Learning Women in Computer Vision Widening NLP Black in AI {Dis}Ability in AI LatinX in AI Queer in AI    Statistics conferences  Caucus for Women In Statistics: offers travel awards for women to attend JSM  Career development, networking, etc. There are many conferences and workshops available for career development at various stages:\n CRA-WP Grace Hopper Celebration Research Scholars Program CRA Grad Cohort Workshop for Women Rising Stars in Computational \u0026amp; Data Sciences Rising Stars in EECS  Additional resources Some additional resources, including recommendations by folks on Twitter:\n Chinasa Okolo\u0026rsquo;s list of CS Graduate Fellowships Rebecca Willett\u0026rsquo;s list of Graduate Student Fellowship Opportunities Women in Machine Learning - Opportunities, tips, and tools  ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"973d075c602b4afd3f4234451e8780c5","permalink":"https://dicai.github.io/post/funding/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/funding/","section":"post","summary":"A list of fellowships, scholarships, grants, conference support, and career development workshops typically for graduate students and postdocs.","tags":[],"title":"Funding Resources for Junior Women in Machine Learning and Statistics","type":"post"},{"authors":["Diana Cai","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"A longer version of this article is available on the arXiv (link).\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7e6bb18949e76a950f04d8284357307d","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning-wkshp/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning-wkshp/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning"],"title":"Weighted meta-learning","type":"publication"},{"authors":["Diana Cai","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"Preliminary version appeared in the ICML 2020 Workshop on AutoML.\nCitation @article{cai2020weighted, title={Weighted Meta-Learning}, author={Cai, Diana and Sheth, Rishit and Mackey, Lester and Fusi, Nicolo}, journal={arXiv preprint arXiv:2003.09465}, year={2020} } ","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"f17dc499b2399d89f59fc82762799ec0","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning","maximum mean discrepancy","task similarity","learning from multiple sources"],"title":"Weighted meta-learning","type":"publication"},{"authors":["Diana Cai","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"8f54f8f5d9cd75a471ee6dc1f5904a0d","permalink":"https://dicai.github.io/publication/conference-paper/2019-finmix/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2019-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["Diana Cai","Michael Mitzenmacher","Ryan P. Adams"],"categories":null,"content":"Citation @inproceedings{cai2018bayesian, title={A Bayesian nonparametric view on count-min sketch}, author={Cai, Diana and Mitzenmacher, Michael and Adams, Ryan P}, booktitle={Advances in Neural Information Processing Systems 31}, pages={8782--8791}, year={2018} } ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"f4640bb3a8b77055dedc594df6347ba5","permalink":"https://dicai.github.io/publication/conference-paper/2018-cmsketch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2018-cmsketch/","section":"publication","summary":"The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data and text data, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.","tags":["probabilistic modeling","count-min sketch","data structures","data streaming","Bayesian nonparametrics"],"title":"A Bayesian nonparametric view on count-min sketch","type":"publication"},{"authors":["Trevor Campbell","Diana Cai","Tamara Broderick"],"categories":null,"content":"Preliminary version in the NeurIPS 2016 Workshop on Practical Bayesian Nonparametrics: [PDF].\nCitation @article{campbell2016exchangeable, title={Exchangeable Trait Allocations}, author={Campbell, Trevor and Cai, Diana and Broderick, Tamara}, journal={Electronic Journal of Statistics}, volume={12}, number={2}, pages={2290 -- 2322}, year={2016} } ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"09795bb0813857f3d7b2c2944638f125","permalink":"https://dicai.github.io/publication/journal-article/2018-traitexch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/journal-article/2018-traitexch/","section":"publication","summary":"Trait allocations are a class of combinatorial structures in which data may belong to multiple groups and may have different levels of belonging in each group. Often the data are also exchangeable, i.e., their joint distribution is invariant to reordering. In clustering—a special case of trait allocation—exchangeability ...","tags":["exchangeability","trait allocations","paintbox representations","Bayesian inference","Bayesian nonparametrics","exchangeable partition probability functions"],"title":"Exchangeable trait allocations","type":"publication"},{"authors":["Diana Cai","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"31d0bb8d44d39562d7b1582f3f39abe8","permalink":"https://dicai.github.io/publication/conference-paper/2017-finmix/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2017-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":null,"categories":null,"content":"Probabilistic models\u0026mdash;a foundation of modern data analysis\u0026mdash;rely on simplifying assumptions of complex real-life phenomena. Crucially, these methods operate on the assumption that the model is correct. Complex models are often necessary to accurately learn about sophisticated real-world phenomena, but even with careful model checking, some amount of model misspecification is inevitable. In some cases, this misspecification may lead to undesirable behavior, such as uninterpretable or even misleading results.\nMy research focuses on studying misspecification in probabilistic models with the goal of understanding when our model assumptions lead to desirable behaviors and when they lead to misleading and uninterpretable inferences. My goal is to develop and understand statistical machine learning models under misspecification and distribution shift and to study the impact on downstream tasks, such as decision making.\nA few current reseach directions for understanding and developing more robust machine learning methods include studying:\n Model misspecification in latent variables, e.g., mixture models, and methods for Bayesian robustness Misspecification in network models including for sparsity and power laws Online changepoint detection for expensive models Multi-source meta-learning and transfer learning Flexible likelihood approximations via kernels in inverse reinforcement learning  See also our workshop at NeurIPS 2021: \u0026ldquo;Your Model is Wrong: Robustness and misspecification in probabilistic machine learning\u0026rdquo;.\n","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"c3f4fad963a39cfcda04ac94f6731594","permalink":"https://dicai.github.io/project/robust/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/robust/","section":"project","summary":"Making probabilistic machine learning more robust to model misspecification","tags":["selected"],"title":"Robust and reliable machine learning","type":"project"},{"authors":["Diana Cai","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"Preliminary versions appeared as:\n Completely random measures for modeling power laws in sparse graphs. NIPS workshop on Networks in the Social and Information Sciences, 2015. Edge-exchangeable graphs and sparsity. NIPS workshop on Networks in the Social and Information Sciences, 2015. Edge-exchangeable graphs, sparsity, and power laws. NIPS Workshop on Bayesian Nonparametrics: The Next Generation, 2015.  Citation Main publication:\n@inproceedings{cai2016edge, title={Edge-exchangeable graphs and sparsity}, author={Cai, Diana and Campbell, Trevor and Broderick, Tamara}, booktitle={Advances in Neural Information Processing Systems 29}, pages={4249--4257}, year={2016} } Workshop papers:\n@inproceedings{cai2015completely, title={Completely random measures for modeling power laws in sparse graphs}, author={Cai, Diana and Broderick, Tamara}, booktitle={NIPS 2015 Workshop on Networks in the Social and Information Sciences}, year={2015} } @inproceedings{broderick2015edge, title={Edge-exchangeable graphs and sparsity}, author={Broderick, Tamara and Cai, Diana}, booktitle={NIPS 2015 Workshop on Networks in the Social and Information Sciences}, year={2015} } @inproceedings{broderick2015edge, title={Edge-exchangeable graphs, sparsity, and power laws}, author={Broderick, Tamara and Cai, Diana}, booktitle={NIPS 2015 Workshop on Bayesian Nonparametrics: The Next Generation}, year={2015} } ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"f63f242ec83a3f8fb078fb6a4cfa31ef","permalink":"https://dicai.github.io/publication/conference-paper/2016-edgeexch/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-edgeexch/","section":"publication","summary":"Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse ...","tags":["graphs","exchangeability","model misspecification","Bayesian nonparametrics"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["Diana Cai","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"This article focuses on the graph paintbox. A full journal article on a more general structure, trait allocations, is available online:\nTrevor Campbell, Diana Cai, Tamara Broderick. Exchangeable trait allocations. Electronic Journal of Statistics, 2018.","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"42b5f37d8c0af560f0e18c00b9797ffc","permalink":"https://dicai.github.io/publication/conference-paper/2016-paintbox/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-paintbox/","section":"publication","summary":"In NeurIPS 2016 Workshop on Adaptive and Scalable Nonparametric Methods in Machine Learning","tags":["workshop"],"title":"Paintboxes and probability functions for edge-exchangeable graphs","type":"publication"},{"authors":["Diana Cai","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"Preliminary version in the NIPS Workshop on Bayesian Nonparametrics, 2015. Additional Notes Keywords:directed graphs, directed exchangeable graphs, directed stochastic block model, directed infinite relational model\nCitation @article{cai2016priors, title={Priors on exchangeable directed graphs}, author={Cai, Diana and Ackerman, Nathanael and Freer, Cameron}, journal={Electronic Journal of Statistics}, volume={10}, number={2}, pages={3490 -- 3515}, year={2015} } ","date":1475280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475280000,"objectID":"2c12d00cc33716d58e025d5e80a0a7de","permalink":"https://dicai.github.io/publication/journal-article/2016-digraphons/","publishdate":"2016-10-01T00:00:00Z","relpermalink":"/publication/journal-article/2016-digraphons/","section":"publication","summary":"Directed graphs occur throughout statistical modeling of networks, and exchangeability is a natural assumption when the ordering of vertices does not matter. There is a deep structural theory for exchangeable undirected graphs, which extends to the directed case via measurable objects known as digraphons ...","tags":["graphs","model misspecification","exchangeability","Bayesian nonparametrics","mixture models"],"title":"Priors on exchangeable directed graphs","type":"publication"},{"authors":null,"categories":null,"content":"In many domains in science, medicine, and engineering, computational methods for simulation, uncertainty quantification, and experimental design are becoming increasingly important. However, a key challenge in these domains is dealing with computationally expensive methods. In some problems, massive and high-dimensional data require algorithms that can scale to those settings. In other problems, accurate models for complex systems are expensive to evaluate, such as an expensive physical simulation, or observations may be expensive to gather.\nI am developing computational methods with the goal of accelerating and improving the performance of approximate Bayesian inference methods and methods for decision making under uncertainty. Some current research directions include:\n Developing multi-fidelity methods for MCMC with expensive target densities, Bayesian optimal experimental design with expensive observations, and online Bayesian changepoint detection with expensive observations Developing general-purpose reparameterization gradient approaches that allow for, e.g., variational Bayesian inference with more flexible model families or sensitivity analysis of a posterior functional Developing methods for data approximation, such as count sketching and random projections Applications to Bayesian experimental design in spatial genomics  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"61eee1408f8b8505b38be41a158c9752","permalink":"https://dicai.github.io/project/inference/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/inference/","section":"project","summary":"Accelerating machine learning algorithms for inference, design, and search","tags":["selected"],"title":"Approximate inference \u0026 decision making","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"39d86bd4b3a289dcaea1e73efa2ec5cf","permalink":"https://dicai.github.io/project/structured/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/structured/","section":"project","summary":"","tags":["graphs"],"title":"Relational data modeling","type":"project"},{"authors":["Diana Cai","Tamara Broderick"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"5966ba801d62831837167b01470b52e0","permalink":"https://dicai.github.io/publication/conference-paper/2015-powerlaws/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-powerlaws/","section":"publication","summary":"","tags":["workshop"],"title":"Completely random measures for modeling power laws in graphs","type":"publication"},{"authors":["Tamara Broderick","Diana Cai"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"3a673293c829c81f71a241a26e5f02b7","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-networks/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-networks/","section":"publication","summary":"In NeurIPS 2015 Workshop on Networks in the Social and Information Sciences","tags":["workshop"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["Tamara Broderick","Diana Cai"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"152aab36a6bdcc794c98f0dd6957ab07","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-bnp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-bnp/","section":"publication","summary":"In NeurIPS 2015 Workshop on Bayesian Nonparametrics","tags":["workshop"],"title":"Edge-exchangeable graphs, sparsity, and power laws","type":"publication"},{"authors":null,"categories":null,"content":"I develop flexible and interpretable nonparametric models and studying their properties, as well as scalable algorithms for these models. Some of the fundamental building blocks for nonparametric Bayesian models include the Dirichlet process and the Gaussian process.\nSome existing work focuses on developing models and algorithms for:\n mixture models with growing number of components graphs and relational data modeling, reducing computational cost in inference in Gaussian processes Bayesian count sketching via nonparametric priors, and modeling with conditional kernel density estimation for inverse reinforcement learning applications  See also our workshop at NeurIPS 2018: All of Bayesian Nonparametrics and References on Bayesian Nonparametrics for additional references.\n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"a3571e5251f66a929b20a69d499d2346","permalink":"https://dicai.github.io/project/bnp/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/bnp/","section":"project","summary":"Developing flexible Bayesian models and studying their properties","tags":["selected"],"title":"Bayesian nonparametric models \u0026 applications","type":"project"},{"authors":["Diana Cai","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"Citation @article{cai2014iterative, title={An iterative step-function estimator for graphons}, author={Cai, Diana and Ackerman, Nathanael and Freer, Cameron}, journal={arXiv preprint arXiv:1412.2129}, year={2014} } ","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"e7e646a2137065633c6f813ad4c06d02","permalink":"https://dicai.github.io/publication/conference-paper/2014-isfe/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2014-isfe/","section":"publication","summary":"arXiv e-print 1412.2129","tags":["preprint"],"title":"An iterative step-function estimator for graphons","type":"publication"},{"authors":["Diana Cai"],"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"b1159f608ebecb41b9c054705c61e309","permalink":"https://dicai.github.io/publication/theses/2014-thesis/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/publication/theses/2014-thesis/","section":"publication","summary":"","tags":["thesis"],"title":"Scalable methods for Bayesian online changepoint detection","type":"publication"}]