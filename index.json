[{"authors":["admin"],"categories":null,"content":"I am a PhD student at Princeton University in Computer Science and am broadly interested in machine learning and statistics. At Princeton, I am advised by Ryan P. Adams and Barbara Engelhardt, and I also work with Tamara Broderick at MIT. I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group.\nPreviously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.S. in Statistics from the University of Chicago, and an M.A. in Computer Science from Princeton University. During the summer of 2019, I was an intern at Microsoft Research New England, where I worked with Nicolo Fusi and Lester Mackey. Currently, I am a member of the Women in Machine Learning Board of Directors. My research is generously supported by a Google PhD Fellowship in Machine Learning.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dicai.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at Princeton University in Computer Science and am broadly interested in machine learning and statistics. At Princeton, I am advised by Ryan P. Adams and Barbara Engelhardt, and I also work with Tamara Broderick at MIT. I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group.\nPreviously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Fellowships, Scholarships, and Grants Below is a list of fellowships, scholarships, and grants for graduate students, typically geared toward women or underrepesented groups in computing:\n Microsoft Ada Lovelace PhD Fellowship: 2nd year PhD students at time of application Microsoft Dissertation Grant: 4th year PhD or above at time of application Google PhD Fellowship: 3rd year or above when Fellowship begins (as of 2020, for North American universities, 2+ nominations need to be students from underrepresented groups) Women Techmakers Scholarship: undergraduate, masters, and PhD students Gertrude M. Cox Scholarship: MS or PhD students Loreal USA for Women in Science: postdoctoral fellowship  Conference support Machine Learning conferences  Google Conference and Travel Scholarships: registration and travel support to attend select conferences NeurIPS and ICML often offer D\u0026amp;I scholarships for underrepresented groups in ML, including women ML/AI affinity groups: often offer travel (or registration) assistance to co-located conferences  Women in Machine Learning Women in Computer Vision Widening NLP Black in AI {Dis}Ability in AI LatinX in AI Queer in AI   Statistics conferences  Caucus for Women In Statistics: offers travel awards for women to attend JSM  Career development, networking, etc.  CRA-WP Grace Hopper Celebration Research Scholars Program CRA Grad Cohor Workshop for Women  ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598227200,"objectID":"e84444c144cb1de166502af4bdab8a03","permalink":"https://dicai.github.io/post/funding/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/funding/","section":"post","summary":"Fellowships, Scholarships, and Grants Below is a list of fellowships, scholarships, and grants for graduate students, typically geared toward women or underrepesented groups in computing:\n Microsoft Ada Lovelace PhD Fellowship: 2nd year PhD students at time of application Microsoft Dissertation Grant: 4th year PhD or above at time of application Google PhD Fellowship: 3rd year or above when Fellowship begins (as of 2020, for North American universities, 2+ nominations need to be students from underrepresented groups) Women Techmakers Scholarship: undergraduate, masters, and PhD students Gertrude M.","tags":[],"title":"Funding Resources for Junior Women in Machine Learning and Statistics","type":"post"},{"authors":["Diana Cai*","Trevor Campbell*","Tamara Broderick"],"categories":null,"content":"NeurIPS 2019 Workshop on Machine Learning with Guarantees [pdf], and also the Symposium on Advances in Approximate Bayesian Inference 2017, co-located with NeurIPS 2017. -- ","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"bb46b446a19a69e0fecb632938cedad6","permalink":"https://dicai.github.io/publication/journal-article/2020-finmix/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/publication/journal-article/2020-finmix/","section":"publication","summary":"Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. Practitioners commonly use a Dirichlet process mixture model (DPMM) for this purpose; in particular, they count the number of clusters---i.e. components containing at least one data point---in the DPMM posterior.  But Miller and Harrison (2013) warn that the DPMM cluster-count posterior is severely inconsistent for the number of latent components when the data are truly generated from a finite mixture; that is, the cluster-count posterior probability on the true generating number of components goes to zero in the limit of infinite data.  A potential alternative is to use a finite mixture model (FMM) with a prior on the number of components.  Past work has shown the resulting FMM component-count posterior is consistent.  But existing results crucially depend on the assumption that the component likelihoods are perfectly specified. In practice, this assumption is unrealistic, and empirical evidence (Miller and Dunson, 2019) suggests that the FMM posterior on the number of components is sensitive to the likelihood choice. In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM posterior on the number of components is _ultraseverely inconsistent_\u0026colon; for any finite k, the posterior probability that the number of components is k converges to 0 in the limit of infinite data.  We illustrate practical consequences of our theory on simulated and real data sets.","tags":["preprint"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7e6bb18949e76a950f04d8284357307d","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning-wkshp/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning-wkshp/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning"],"title":"Weighted meta-learning","type":"publication"},{"authors":["admin","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"f17dc499b2399d89f59fc82762799ec0","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning"],"title":"Weighted meta-learning","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"8f54f8f5d9cd75a471ee6dc1f5904a0d","permalink":"https://dicai.github.io/publication/conference-paper/2019-finmix/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2019-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Michael Mitzenmacher","Ryan P. Adams"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"f4640bb3a8b77055dedc594df6347ba5","permalink":"https://dicai.github.io/publication/conference-paper/2018-cmsketch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2018-cmsketch/","section":"publication","summary":"The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data and text data, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.","tags":["probmodeling"],"title":"A Bayesian nonparametric view on count-min sketch","type":"publication"},{"authors":["Trevor Campbell","admin","Tamara Broderick"],"categories":null,"content":"Preliminary version in the NIPS 2016 Workshop on Practical Bayesian Nonparametrics, 2016. PDF\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"09795bb0813857f3d7b2c2944638f125","permalink":"https://dicai.github.io/publication/journal-article/2018-traitexch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/journal-article/2018-traitexch/","section":"publication","summary":"In EJS 2018","tags":["publication"],"title":"Exchangeable trait allocations","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"31d0bb8d44d39562d7b1582f3f39abe8","permalink":"https://dicai.github.io/publication/conference-paper/2017-finmix/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2017-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"Preliminary versions appeared as:   Completely random measures for modeling power laws in sparse graphs. NIPS workshop on Networks in the Social and Information Sciences, 2015.   Edge-exchangeable graphs and sparsity. NIPS workshop on Networks in the Social and Information Sciences, 2015.   Edge-exchangeable graphs, sparsity, and power laws. NIPS Workshop on Bayesian Nonparametrics: The Next Generation, 2015.  \n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"f63f242ec83a3f8fb078fb6a4cfa31ef","permalink":"https://dicai.github.io/publication/conference-paper/2016-edgeexch/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-edgeexch/","section":"publication","summary":"In NeurIPS 2016","tags":["publication"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"This article focuses on the graph paintbox. A full journal article on a more general structure, trait allocations, is available online: Trevor Campbell, Diana Cai, Tamara Broderick. Exchangeable trait allocations. Electronic Journal of Statistics, 2018.\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"42b5f37d8c0af560f0e18c00b9797ffc","permalink":"https://dicai.github.io/publication/conference-paper/2016-paintbox/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-paintbox/","section":"publication","summary":"In NeurIPS 2016 Workshop on Adaptive and Scalable Nonparametric Methods in Machine Learning","tags":["workshop"],"title":"Paintboxes and probability functions for edge-exchangeable graphs","type":"publication"},{"authors":["admin","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"Preliminary version in the NIPS Workshop on Bayesian Nonparametrics, 2015. \n","date":1475280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475280000,"objectID":"2c12d00cc33716d58e025d5e80a0a7de","permalink":"https://dicai.github.io/publication/journal-article/2016-digraphons/","publishdate":"2016-10-01T00:00:00Z","relpermalink":"/publication/journal-article/2016-digraphons/","section":"publication","summary":"In EJS 2016","tags":["publication"],"title":"Priors on exchangeable directed graphs","type":"publication"},{"authors":null,"categories":null,"content":"Hello this is my project description page.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c3f4fad963a39cfcda04ac94f6731594","permalink":"https://dicai.github.io/project/robust/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/robust/","section":"project","summary":"Hello this is my project description page.","tags":null,"title":"Probabilistic modeling and robust ML","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"39d86bd4b3a289dcaea1e73efa2ec5cf","permalink":"https://dicai.github.io/project/structured/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/structured/","section":"project","summary":"","tags":["graphs"],"title":"Relational data modeling","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b5e16b8b536e2957d2637ade6db22a49","permalink":"https://dicai.github.io/project/constrained/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/constrained/","section":"project","summary":"","tags":["graphs"],"title":"Resource-constrained ML","type":"project"},{"authors":["admin","Tamara Broderick"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"5966ba801d62831837167b01470b52e0","permalink":"https://dicai.github.io/publication/conference-paper/2015-powerlaws/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-powerlaws/","section":"publication","summary":"","tags":["workshop"],"title":"Completely random measures for modeling power laws in graphs","type":"publication"},{"authors":["Tamara Broderick","admin"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"3a673293c829c81f71a241a26e5f02b7","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-networks/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-networks/","section":"publication","summary":"In NeurIPS 2015 Workshop on Networks in the Social and Information Sciences","tags":["workshop"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["Tamara Broderick","admin"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"152aab36a6bdcc794c98f0dd6957ab07","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-bnp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-bnp/","section":"publication","summary":"In NeurIPS 2015 Workshop on Bayesian Nonparametrics","tags":["workshop"],"title":"Edge-exchangeable graphs, sparsity, and power laws","type":"publication"},{"authors":["admin","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"e7e646a2137065633c6f813ad4c06d02","permalink":"https://dicai.github.io/publication/conference-paper/2014-isfe/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2014-isfe/","section":"publication","summary":"arXiv e-print 1412.2129","tags":["preprint"],"title":"An iterative step-function estimator for graphons","type":"publication"},{"authors":["admin"],"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"b1159f608ebecb41b9c054705c61e309","permalink":"https://dicai.github.io/publication/theses/2014-thesis/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/publication/theses/2014-thesis/","section":"publication","summary":"","tags":["thesis"],"title":"Scalable methods for Bayesian online changepoint detection","type":"publication"}]