[{"authors":["admin"],"categories":null,"content":"I am a PhD student at Princeton University in Computer Science and am broadly interested in machine learning and statistics. At Princeton, I am advised by Ryan P. Adams and Barbara Engelhardt, and I also work with Tamara Broderick at MIT. I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group.\nPreviously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.S. in Statistics from the University of Chicago, and an M.A. in Computer Science from Princeton University. During the summer of 2019, I was an intern at Microsoft Research New England, where I worked with Nicolo Fusi and Lester Mackey. Currently, I am a member of the Women in Machine Learning Board of Directors. My research is generously supported by a Google PhD Fellowship in Machine Learning.\n","date":1593561600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1593561600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dicai.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at Princeton University in Computer Science and am broadly interested in machine learning and statistics. At Princeton, I am advised by Ryan P. Adams and Barbara Engelhardt, and I also work with Tamara Broderick at MIT. I am a member of the Laboratory for Intelligent Probabilistic Systems Group and the Biological and Evolutionary Explorations using Hierarchical Integrative Statistical Models Group.\nPreviously, I received an A.B. in Computer Science and Statistics from Harvard University, an M.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":" Starting a PhD in itself is full of many challenges, but many students starting this fall will have to navigate the extra hurdle of a virtual environment along with many other challenges. Here I’m attempting to write down what I’d tell my junior colleagues starting a PhD in the US \u0026ndash; geared toward students in Computer Science, and especially Machine Learning \u0026ndash; including some Princeton-specific thoughts.\nTable of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nNavigating the academic landscape In graduate school, you get an immense amount of freedom to choose how to spend your time. Figuring out how to weigh and adjust your priorities is a skill that will be developed over time.\nGetting started with research At Princeton, in your first year or so, you generally are free to explore working with different professors before committing to an official advisor (or co-advisors) \u0026ndash; take advantage of this early on, but also throughout your PhD.\nUsually your advisor will help you get started with a new research project and suggest a few directions to get started with; maybe your advisor will want you to come in with a few ideas of your own and help you shape those ideas. Try to schedule regular meetings if possible, especially if you find this helpful or motivating (this is usually an aspect of “advisor style” to keep in mind).\nIn practice, advisors are fairly busy and generally expect a lot of independence from you as a student (but you should ask your advisor directly what their expectations are!). To supplement whatever guidance you receive for a research project, some other great things I’d suggest trying are:\n Try to join a project with a senior PhD student (or postdoc) that has a few publications already: one way to develop strong research skills is by working with someone who has already published a few impactful papers. This will generally help you get up to speed on, e.g., what background material to learn, how to run experiments on the cluster, how to write a paper, what are the typical venues to submit to, what are some future extensions, how to choose research projects, how to write a paper rebuttal, how to submit to the arXiv, etc. Get to know other professors in your department/area: these may be eventual committee members, co-advisors, or collaborators. How do you get to know other professors? Ask questions in class and attend office hours, take seminars from professors, ask faculty members if you can sit in their group meetings or reading groups. Get advice on what papers, books, and resources to check out from other students and postdocs. But also try to avoid groupthink and figure out what problems excite you, rather than gravitating to hot areas by default. Attend relevant seminars and talks to get an idea of what types of research people are currently thinking about. Jot down the main takeaway of the talk, and make a mental note if there is a paper or preprint online.  I’d also suggest keeping a (digital) notebook of research ideas and adding to it every time inspiration strikes. Many of these will probably be “bad” ideas, but sometimes even bad ideas can get shaped into solid research contributions.\nLastly, I think it\u0026rsquo;s important to focus on building research skills, rather than worrying too much about working on ambitious research projects. Your PhD research will help you contribute new knowledge to some area of research, but does not define what you work on for the rest of your career!\nCheck out:\n Matt Might\u0026rsquo;s Illustrated Guide to a PhD. Nick Feamster - Do You Need a Ph.D.?  Balancing classes (breadth \u0026amp; foundations) vs research (depth) Over time, I’ve come to see coursework as an opportunity to learn from someone who is an expert in some area and as a chance to really expand your toolkit. So I’d recommend trying to take some foundational courses in your earlier years if you can, especially if you haven’t had much previous exposure to particular topics, as well as some topics courses to help expand your breadth. Given the virtual format: I\u0026rsquo;d even suggest enrolling in more courses than you might normally consider (e.g., 2 or 3), just because it adds some much-needed extra structure during a virtual term.\nAt Princeton, some excellent foundational courses I’d recommend for ML students (in CS, ORFE, and EE) include:\n Theoretical Machine Learning Probability Theory Probability in High-Dimension (and/or Mathematics of Data Science) Statistical Inference (and/or Nonparametric Statistics) Convex Optimization (and/or Nonlinear Optimization and/or Computational Statistics)  On seminars: I have some ambivalence about taking research seminars, especially paper reading and presentation courses (I have taken many such courses). But I think the right ones can be a really great way to learn about a narrow topic relevant to your research and to get to know a professor in a specific research area you’re interested in.\nFor example, at Princeton I really enjoyed taking a seminar on Optimization for Machine Learning. I liked this because it was a lecture-based course covering both foundational topics and modern research progress. The course had a few simple assignments to help reinforce my understanding of the material but were also not too time consuming.\nMy suggestion is to only take a few seminars, and to wait until later in your PhD when you have less time for more intensive coursework but still want to gain breadth in some new areas. If there is a topic that you’re really interested in or is your specific research area and you see a seminar being taught, then of course I’d recommend checking it out, since these aren’t usually taught every year!\nThe CS program at Princeton gives you a lot of flexibility of when you can take courses and which courses to take. Thus, this give you some choice in how to balance your course schedule with research. For my required breadth areas, I took Advanced Algorithms and Networks. During my first 3 years, I chose to take one semester that was more course-focused (e.g., one or more courses with weekly or bi-weekly assignments) and one semester that was more research-focused (e.g., at most 1 research seminar course), and I’d typically shoot for some type of paper submission deadline during the term when I wasn’t doing much coursework (RIP Spring 2020).\nBuilding communication skills Communication skills are crucial to develop for researchers. We often hear about communication skills in giving talks and writing papers (which is very important!). Early on in your graduate career, you’ll also want to think about how to have effective meetings \u0026ndash; this is especially important given the virtual nature of meetings right now.\nLeading effective research meetings Maybe you\u0026rsquo;ve gotten some awesome results for a project, or maybe you\u0026rsquo;re stuck on something and are meeting with your advisor and/or collaborators. Beyond just preparing the technical content of what you’re planning to discuss, think about how to communicate your results in a clear and efficient manner so as to minimize your collaborators\u0026rsquo; cognitive load and ultimately get the best feedback possible.\nFor in-person (or video call) meetings, you may want to experiment with coming in with an agenda of items you want to discuss (or even consider sending this out in advance). If you’re presenting some results, remind your collaborators of the setting and your last conversation(s) before sharing results or updates; e.g., don’t just screen share and jump into page 4 of your notes \u0026ndash; briefly remind everyone of what\u0026rsquo;s going on.\nIf you are collaborating with someone through, e.g., email or Slack, you\u0026rsquo;ll want to provide a little bit more context of what you\u0026rsquo;re trying to communicate. For example, don’t just send them a figure with little to no context, and don’t assume they remember the details of your last conversation. At the minimum a concise writeup of the setting is useful, for instance:\n What data are you using (if synthetic - how was it generated?) What model are you using and how was it fit? Hyperparameters? What is a summary of the conclusions/takeaways you want to convey? What questions do you have? What are you stuck on? What are you thinking about doing next?  Read over your writeup and double check that it clearly and precisely communicates the points you\u0026rsquo;re trying to convey. Make sure any plots you include are clearly labeled. Keep things short to maximize the chance it\u0026rsquo;ll actually get read.\nKeep detailed notes of all of your meetings \u0026ndash; this is especially important if you start working on multiple projects and need to context switch. At the end of a meeting, sometimes it’s helpful to verbally discuss concretely what your TODOs are for next time. This may help you focus on what to do next and also help get some direction from your advisor/collaborators.\n1-1 meetings with your advisor Here\u0026rsquo;s a great zine from Julia Evans \u0026ndash; it\u0026rsquo;s geared towards industry, but I think the main ideas apply in terms of meetings with your advisor. Besides the technical specifics of a research project, you should discuss other things like:\n What should I read to learn about [topic]? I\u0026rsquo;d like to brainstorm about [potential research direction]. What should I expect for my [generals exam, defense, conference talk]? I want apply for [fellowship, award, internship, contributed talk]. How do I do [write a research statement, give a talk, write a paper]? I think we could improve [the reading group] by doing [suggestion]. Do you have any feedback on [the talk I gave, my research statement]? What are the typical job trajectories after the PhD? --  Writing and presentation skills Writing and presentation skills are often under-emphasized in graduate school training but are so crucial for getting paper acceptances, communicating your ideas, and getting funding for new research.\nIn my experience, the best way to get better at giving talks and writing papers is to get a lot of practice in. Keeping a research blog (or detailed notes) can help you practice how to clearly articulate a technical topic. Besides advisors and collaborators, you can also get feedback on paper drafts from peers \u0026ndash; this feedback gives you perspective on potential comments you may get from reviewers or readers that you can work on clarifying.\nPractice giving talks and presentations as much as possible, and get feedback on what could be more clear \u0026ndash; group meetings, reading groups, and class presentations are a great way to get started with this. When you watch a talk you really like, make a note of what was great about it. If you see a talk you thought could have been better, make a note of why. Later on, you\u0026rsquo;ll give talks at conferences and workshops and at other universities, and it\u0026rsquo;s great to be extra prepared before you start speaking in front of larger audiences.\nPractice your talks before you give them! This will help you will help with your general delivery and flow, even when you get nervous. Before you give a generals or qualifying exam talk, schedule a practice talk with your peers and/or ask your advisor if you can give a practice talk.\nAdditional reading:\n Margo Seltzer\u0026rsquo;s Tips on Writing on Thesis Jacob Steinhardt\u0026rsquo;s Advice for Authors Lipton and Steinhardt - Trouble Trends in Machine Learning Scholarship Trevor Campbell on \u0026ldquo;How to Explain Things\u0026rdquo;  Navigating the literature As a graduate student, you’ll quickly realize how many papers there are: journal papers, conference papers, workshop papers, arXiv papers, etc. You might wonder how to possibly keep up with all the papers being put out every day. My answer to this is: you don’t \u0026ndash; but more on this later.\nSome ways to read papers include:\n High-level browsing: reading the title, authors, and (maybe) abstract Big-picture read: high-level browsing + reading the introduction and skimming the sections of the paper. Detailed read: big-picture read + carefully reading (and re-reading) many or all of the technical sections of the paper  My workflow: Most papers I read only get a big-picture read. I also do a lot of high-level browsing of papers from daily arXiv email blasts (I’m subscribed to stat.ML and math.ST), and I generally make a note of anything directly relevant to my research. If it’s not too relevant to my work but seems interesting, I try to save it using the Google Keep browser plugin.\nTypically papers I read carefully are longer, journal articles and take a lot of time for me to digest; these are usually papers directly related to a research project I’m currently working on or papers that provide a deep understanding of a new topic I\u0026rsquo;m trying to understand. Also, if there\u0026rsquo;s a talk available (e.g., from a conference, workshop, tutorial, or summer school), I try to watch that before reading the paper in depth; that way I come in already knowing the main idea and concepts before diving into the details.\nTips for early on: keep a detailed note-tracking system for papers, and have these backed up via cloud storage or version control. You may even consider keeping a citation system. When you do a big-picture read, it’s helpful to write down your version of the big picture summary, so you can refer back to it later. This is especially useful when it’s time to write up a paper and you’re going back to review and organize related work. Taking notes and revisiting them upon second or third readings of a paper is also useful for gaining new insights on a topic \u0026ndash; oftentimes when you’re new to a topic, you don’t have as much context or background to fully understand some parts of a paper.\nI am generally skeptical of tools that you have to pay a monthly subscription to use, as I value persistence of my notes and workflow (and sometimes tools stop being supported or start charging for more than two devices). Here are some tools I personally use as of the current writing:\n For detailed research notes: I like to have these notes in LaTeX + pdf format and in backed up in a git repository. Notability for iOS and MacOS: I’ve been using my ipad to keep a lot of notes on each of my projects, meeting notes, notes from talks and lectures, and paper annotation notes. Recently I also got the desktop app in order to make it easier to add papers and typed notes. Google Keep for super quick notes: e.g., I save papers that might be relevant after doing a high-level browse using the browser extension. I keep detailed folders of papers (in Dropbox) to allow for fast search of titles/authors. Using Jekyll/Hugo (or similar tool) + MathJax + git to keep LaTeX notes in a blog format: this makes it nice to organize notes by topic (or give some notes multiple topics) and to in general customize your note-taking format. Note: you can just organize this locally without posting your notes online. Google docs: for collaborative projects, sometimes I keep lists of related work with notes in shared google docs / folders. Paper notebooks: I do like to write on paper, but I try to make sure to transfer anything important to LaTeX or some other digital notebook so that I don’t lose it.  Find a format that works for you so that you can refer back to them in 5 years, and keep those notes under version control or backed up in some way.\nAside: when I first started doing ML research, I kept most of my notes in a paper notebook. Now I love some good Muji products \u0026ndash; but I found that I rarely refer back to handwritten notes and eventually lose track of physical pages and notebooks. Since switching over to more digital notetaking tools, I\u0026rsquo;ve definitely noticed a difference in my ability to keep track of multiple projects and ideas.\nBuilding strong relationships Finding a good set of collaborators is often key and can make research more fun. Learning from a diverse set of people often helps you develop new perspectives and leads to more creative thinking. Finding good collaborators can take time (and is often more difficult for students from underrepresented groups), but one way to get started is to first build your research network.\nVirtual coffee chats Given the virtual environment, building community will be even harder than before, so it’s important to stay proactive. I’d highly recommend trying to schedule a weekly 1-1 coffee chat with other students and postdocs in your lab and in your department. This could lead to friendships and collaborations but also just keep you engaged in the community.\n1-1 meetings with visitors Usually your advisor will also have visiting researchers in the lab and have students sign up to meet with these researchers. This is another great opportunity to get to know another researcher and to share your own research. If you’re not sure what to talk about, some general topics include:\n General introductions about your backgrounds Ask about what they’re currently excited about Tell them a bit about your current interests or a project you’re working on  Generally, researchers and academics love to talk about their own work, and so if you feel like you don’t have much to say yourself, this is an easy way to fill a meeting. But I’d also encourage you to chat about some of your interests \u0026ndash; part of the graduate school experience is learning how to get out of your comfort zone and building up your confidence; it’s totally fine to say you just got started with research but are really interested in X because of Y.\nHave a website and update it regularly Graduate students (and yes, first years too) should make a research website. You can usually host on a university domain or on Github pages. If your department has a student directory, they may ask to link to a personal website \u0026ndash; definitely take advantage of this! Similarly, advisors often link to their current students’ websites.\nHaving a website earlier on in your graduate career is an important step toward having some visibility. When you’re applying for jobs (or internships, fellowships, travel grants, etc.), it is helpful to have a website. It’s also very common to look up someone’s website when considering speaker invitations for events or reviewer invitations for academic conferences and workshops. It’s generally considered a professional must-have now, just like how you need to maintain a CV or resume.\nMany students are hesitant about making a website, often due to feeling like they don\u0026rsquo;t have anything to put online. But even if you feel this way, you can still start out with a fairly minimal website: e.g., name, email, a brief bio (what program are you in, where were you before that), and possibly even a few sentences on what you’re interested in now.\nIf you’re comfortable, here are some ideas on other things you might pick and choose from when deciding what to include:\n A professional photo A brief bio: including who you\u0026rsquo;re advised by, if you\u0026rsquo;re part of a larger research group Links to: Github, Google Scholar, Twitter, CV, etc. Past work and teaching experience Research portfolio: pre-prints and working papers, publications, the report for a course project you’re proud of, a description and link of any software packages you’re written Non-academic work you’re proud of (e.g., volunteer work)  Virtual conferences and workshops Attending conferences is a good way to meet people and learn about new research. One of the advantages of the virtual format is that student registration fees are generally fairly low (~$25) or even free, and so your advisor will likely be more willing to support you, even if you\u0026rsquo;re not presenting any work. You can also apply for a registration fee scholarship \u0026ndash; often conferences offer applications for students as well as researchers from underrepresented groups. In machine learning, several of the major conferences have some presence from some affinity group, such as Black in AI, LatinX in AI, {Dis}Ability in AI, and Women in Machine Learning (also other areas such as NLP, CV), and many of these also offer scholarships to register for conferences, such as NeurIPS and ICML.\nRegistering for a virtual conference gives you access to a bunch of recorded talks and panels, as well as a chance to interact with other conference attendees. I’d suggest trying to attend a virtual conference this year, regardless of whether you\u0026rsquo;re presenting anything.\nWhat are the main conference venues in machine learning? Just to list a few:\n NeurIPS ICML AISTATS COLT UAI ICLR AAAI KDD  Some of these are more geared towards certain areas, such as theory or applications. If you’re working at the intersection of ML and statistics, attending statistics conferences (which generally don’t have published proceedings) may also be of interest, e.g.: JSM, ISBA, or smaller workshops in specific topics.\nA great way to participate in conferences early on is to submit to workshops. Typically, many of the larger conferences (e.g., NeurIPS and ICML) offer a few days of workshops that are typically solicit extended abstracts and short papers (e.g., around 2\u0026ndash;4 pages) that are not published in the conference proceedings. This is a great way to meet other people working in this subfield and to get feedback on your work. Typically, the acceptance rate for workshops is much higher than for a conference (but you should still strive to only submit high-quality work!).\nAdditionally, many research institutes often have semester-long or year-long programs with workshops, such as the Simon’s Institute or the Institute of Advanced Study.\nMaintaining and contributing to a healthy work environment Based on my own personal experience and conversations with peers, there are many challenging aspects of grad school (which I’d say is an understatement!), and these will be different for everyone!\nThe main thing I want to emphasize is that it’s important to maintain and contribute to a healthy work environment, and this will probably take some experimenting to figure out what works for you.\nWFH productivity and mental health It’s a huge privilege for computer scientists to be able to work from home on their computers. That said, I’ve definitely found it hard to be productive working from home full-time in the middle of a pandemic. I think it\u0026rsquo;s important to be especially gentle with ourselves and others right now \u0026ndash; personally, I\u0026rsquo;ve been trying to take things one week at a time.\nPhysical and mental health Take care of your physical and mental health \u0026ndash; this is not easy and takes active work! Graduate school itself is filled with moments of imposter syndrome, failure, isolation, and stress. So being proactive about building up your physical and mental health is crucial for preparing you for potential tough situations:\n Get lots of sleep, drink lots of water, and eat healthily. Try to prioritize getting some exercise and spending some time outdoors every day. Establish a strong support group \u0026ndash; talk to a therapist, your mentors, and friends and family regularly. (At Princeton: check out the CS Grad and GSG Slack Groups.) Develop and maintain some hobbies outside of work. Try to maintain a regular sleep schedule and daily routine.  Work from home tips Try to get a good work setup at home, if possible. You might want to consider if your advisor, department, or university has resources in helping you get set up to work from home, with e.g., a laptop, monitor, desk, and chair. I’d also check with your advisor (or department) to see if they have funds for a tablet for online meetings \u0026ndash; if not, I’ve found that paper+pen and webcam can also be useful for communicating in online video calls. Try to maintain some separation of work and life, and try to keep your work area clean!\nStay organized \u0026ndash; right now the extra lack of structure and interactions makes it easy for the days to just blend together. These days I really rely on calendar invites to keep track of what I need to do. I\u0026rsquo;ve tried a lot of various organization tools. Currently, I\u0026rsquo;ve been using a super simple system \u0026ndash; I maintain a note with checkboxes of all the tasks I need to do this semester and when I need to do them by. Then I make sure to check it every day to remind myself what needs to get done soon and update the list when new tasks come up.\nTake a break! Some days are not going to be that productive \u0026ndash; and that’s ok! Take time off if you need to, but also when you don’t need it so you don’t burn out. In academia, there is a culture of overworking and projecting the image of working all the time, and this is something I actively try to resist. During the semester, I almost always take fall and spring break off and take at least a week off during summer/winter holidays. I also try to not work on the weekends and evenings and generally push back now when people try to schedule work meetings on the weekends or during the evening*.\n*Except perhaps around conference deadlines. :-)\nContributing to an inclusive work environment Machine learning (CS / STEM) has a diversity problem. But as graduate students, we each can strive toward improving the inclusivity of our local community, such as our research groups, departments, university, and the greater research community, and to educate ourselves about systemic issues faced by students and researchers from underrepresented groups. For students starting just now, it won’t be long before you become one of the senior members of your research lab and other students will look to you for guidance.\nI’ll close with some recommended reading:\n Combating Anti-Blackness in the AI Community (Devin Guillory) Diversity \u0026amp; Inclusion Resources from Queer in AI CRA Taulbee Survey [PDF Report link] Creating a Positive Lab Culture (Margo Seltzer) NeurIPS 2018 Diversity \u0026amp; Inclusion Survey (Hal Daume III and Katherine Heller) CRA-W Best Practices: Guidelines for running an inclusive conference Gender bias calculator  ","date":1598745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598832000,"objectID":"91bb1a5d0475fd87c1232b45e26a21b0","permalink":"https://dicai.github.io/post/tips-new-phd-students/","publishdate":"2020-08-30T00:00:00Z","relpermalink":"/post/tips-new-phd-students/","section":"post","summary":"Starting a PhD in itself is full of many challenges, but many students starting this fall will have to navigate the extra hurdle of being in a virtual environment along with many other challenges. Here I’m attempting to write down what I’d tell my junior colleagues starting a PhD -- geared toward students in Computer Science, and especially Machine Learning -- including some Princeton-specific thoughts.","tags":null,"title":"Tips for New PhD Students in Machine Learning","type":"post"},{"authors":[],"categories":null,"content":" Fellowships, Scholarships, and Grants Below is a list of fellowships, scholarships, and grants for graduate students, typically geared toward women or underrepesented groups in computing:\n Microsoft Ada Lovelace PhD Fellowship: 2nd year PhD students at time of application Microsoft Dissertation Grant: 4th year PhD or above at time of application Women Techmakers Scholarship: undergraduate, masters, and PhD students Gertrude M. Cox Scholarship: MS or PhD students Adobe Research Women-in-Technology Scholarship: undergraduates and masters students Loreal USA for Women in Science: postdoctoral fellowship  Conference support Many organizations offer travel awards for graduate students and postdocs to attend conferences. Now that many conferences are virtual, conference support is more commonly taking the form of registration support.\nMachine Learning conferences  Google Conference and Travel Scholarships: registration and travel support to attend select conferences NeurIPS and ICML often offer D\u0026amp;I scholarships for underrepresented groups in ML, including women ML/AI affinity groups: often offer travel (or registration) assistance to co-located conferences  Women in Machine Learning Women in Computer Vision Widening NLP Black in AI {Dis}Ability in AI LatinX in AI Queer in AI   Statistics conferences  Caucus for Women In Statistics: offers travel awards for women to attend JSM  Career development, networking, etc. There are many conferences and workshops available for career development at various stages:\n CRA-WP Grace Hopper Celebration Research Scholars Program CRA Grad Cohort Workshop for Women Rising Stars in Computational \u0026amp; Data Sciences Rising Stars in EECS  Additional resources Some additional resources, including recommendations by folks on Twitter:\n Chinasa Okolo\u0026rsquo;s list of CS Graduate Fellowships Rebecca Willett\u0026rsquo;s list of Graduate Student Fellowship Opportunities Women in Machine Learning - Opportunities, tips, and tools  ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"973d075c602b4afd3f4234451e8780c5","permalink":"https://dicai.github.io/post/funding/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/funding/","section":"post","summary":"A list of fellowships, scholarships, grants, conference support, and career development workshops typically for graduate students and postdocs.","tags":[],"title":"Funding Resources for Junior Women in Machine Learning and Statistics","type":"post"},{"authors":["Diana Cai*","Trevor Campbell*","Tamara Broderick"],"categories":null,"content":"NeurIPS 2019 Workshop on Machine Learning with Guarantees [pdf], and also the Symposium on Advances in Approximate Bayesian Inference 2017, co-located with NeurIPS 2017. -- ","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"bb46b446a19a69e0fecb632938cedad6","permalink":"https://dicai.github.io/publication/journal-article/2020-finmix/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/publication/journal-article/2020-finmix/","section":"publication","summary":"Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. Practitioners commonly use a Dirichlet process mixture model (DPMM) for this purpose; in particular, they count the number of clusters---i.e. components containing at least one data point---in the DPMM posterior.  But Miller and Harrison (2013) warn that the DPMM cluster-count posterior is severely inconsistent for the number of latent components when the data are truly generated from a finite mixture; that is, the cluster-count posterior probability on the true generating number of components goes to zero in the limit of infinite data.  A potential alternative is to use a finite mixture model (FMM) with a prior on the number of components.  Past work has shown the resulting FMM component-count posterior is consistent.  But existing results crucially depend on the assumption that the component likelihoods are perfectly specified. In practice, this assumption is unrealistic, and empirical evidence (Miller and Dunson, 2019) suggests that the FMM posterior on the number of components is sensitive to the likelihood choice. In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM posterior on the number of components is _ultraseverely inconsistent_\u0026colon; for any $k \\in \\mathbb{N}$, the posterior probability that the number of components is $k$ converges to 0 in the limit of infinite data.  We illustrate practical consequences of our theory on simulated and real data sets.","tags":["preprint"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7e6bb18949e76a950f04d8284357307d","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning-wkshp/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning-wkshp/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning"],"title":"Weighted meta-learning","type":"publication"},{"authors":["admin","Rishit Sheth","Lester Mackey","Nicolo Fusi"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"f17dc499b2399d89f59fc82762799ec0","permalink":"https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/conference-paper/2020-weighted-meta-learning/","section":"publication","summary":"Meta-learning leverages related source tasks to learn an initialization that can be quickly fine-tuned to a target task with limited labeled examples.  However, many popular meta-learning algorithms, such as model-agnostic meta-learning (MAML), only assume access to the target samples for fine-tuning. In this work, we provide a general framework for meta-learning based on weighting the loss of different source tasks, where the weights are allowed to depend on the target samples. In this general setting, we provide upper bounds on the distance of the weighted empirical risk of the source tasks and expected target risk in terms of an integral probability metric (IPM) and Rademacher complexity, which apply to a number of meta-learning settings including MAML and a weighted MAML variant.  We then develop a learning algorithm based on minimizing the error bound with respect to an empirical IPM, including a weighted MAML algorithm, alpha-MAML.  Finally, we demonstrate empirically on several regression problems that our weighted meta-learning algorithm is able to find better initializations than uniformly-weighted meta-learning algorithms, such as MAML.","tags":["meta-learning"],"title":"Weighted meta-learning","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"8f54f8f5d9cd75a471ee6dc1f5904a0d","permalink":"https://dicai.github.io/publication/conference-paper/2019-finmix/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2019-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Michael Mitzenmacher","Ryan P. Adams"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"f4640bb3a8b77055dedc594df6347ba5","permalink":"https://dicai.github.io/publication/conference-paper/2018-cmsketch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2018-cmsketch/","section":"publication","summary":"The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data and text data, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.","tags":["probmodeling"],"title":"A Bayesian nonparametric view on count-min sketch","type":"publication"},{"authors":["Trevor Campbell","admin","Tamara Broderick"],"categories":null,"content":"Preliminary version in the NIPS 2016 Workshop on Practical Bayesian Nonparametrics, 2016. PDF\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"09795bb0813857f3d7b2c2944638f125","permalink":"https://dicai.github.io/publication/journal-article/2018-traitexch/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/journal-article/2018-traitexch/","section":"publication","summary":"In EJS 2018","tags":["publication"],"title":"Exchangeable trait allocations","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"31d0bb8d44d39562d7b1582f3f39abe8","permalink":"https://dicai.github.io/publication/conference-paper/2017-finmix/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2017-finmix/","section":"publication","summary":"","tags":["workshop"],"title":"Finite mixture models are typically inconsistent for the number of components","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"Preliminary versions appeared as:   Completely random measures for modeling power laws in sparse graphs. NIPS workshop on Networks in the Social and Information Sciences, 2015.   Edge-exchangeable graphs and sparsity. NIPS workshop on Networks in the Social and Information Sciences, 2015.   Edge-exchangeable graphs, sparsity, and power laws. NIPS Workshop on Bayesian Nonparametrics: The Next Generation, 2015.  \n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"f63f242ec83a3f8fb078fb6a4cfa31ef","permalink":"https://dicai.github.io/publication/conference-paper/2016-edgeexch/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-edgeexch/","section":"publication","summary":"In NeurIPS 2016","tags":["publication"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["admin","Trevor Campbell","Tamara Broderick"],"categories":null,"content":"This article focuses on the graph paintbox. A full journal article on a more general structure, trait allocations, is available online: Trevor Campbell, Diana Cai, Tamara Broderick. Exchangeable trait allocations. Electronic Journal of Statistics, 2018.\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"42b5f37d8c0af560f0e18c00b9797ffc","permalink":"https://dicai.github.io/publication/conference-paper/2016-paintbox/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2016-paintbox/","section":"publication","summary":"In NeurIPS 2016 Workshop on Adaptive and Scalable Nonparametric Methods in Machine Learning","tags":["workshop"],"title":"Paintboxes and probability functions for edge-exchangeable graphs","type":"publication"},{"authors":["admin","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"Preliminary version in the NIPS Workshop on Bayesian Nonparametrics, 2015. \n","date":1475280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475280000,"objectID":"2c12d00cc33716d58e025d5e80a0a7de","permalink":"https://dicai.github.io/publication/journal-article/2016-digraphons/","publishdate":"2016-10-01T00:00:00Z","relpermalink":"/publication/journal-article/2016-digraphons/","section":"publication","summary":"In EJS 2016","tags":["publication"],"title":"Priors on exchangeable directed graphs","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c3f4fad963a39cfcda04ac94f6731594","permalink":"https://dicai.github.io/project/robust/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/robust/","section":"project","summary":"","tags":null,"title":"Probabilistic modeling and robust ML","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"39d86bd4b3a289dcaea1e73efa2ec5cf","permalink":"https://dicai.github.io/project/structured/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/structured/","section":"project","summary":"","tags":["graphs"],"title":"Relational data modeling","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b5e16b8b536e2957d2637ade6db22a49","permalink":"https://dicai.github.io/project/constrained/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/constrained/","section":"project","summary":"","tags":["graphs"],"title":"Resource-constrained ML","type":"project"},{"authors":["admin","Tamara Broderick"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"5966ba801d62831837167b01470b52e0","permalink":"https://dicai.github.io/publication/conference-paper/2015-powerlaws/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-powerlaws/","section":"publication","summary":"","tags":["workshop"],"title":"Completely random measures for modeling power laws in graphs","type":"publication"},{"authors":["Tamara Broderick","admin"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"3a673293c829c81f71a241a26e5f02b7","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-networks/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-networks/","section":"publication","summary":"In NeurIPS 2015 Workshop on Networks in the Social and Information Sciences","tags":["workshop"],"title":"Edge-exchangeable graphs and sparsity","type":"publication"},{"authors":["Tamara Broderick","admin"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"152aab36a6bdcc794c98f0dd6957ab07","permalink":"https://dicai.github.io/publication/conference-paper/2015-edgeexch-bnp/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2015-edgeexch-bnp/","section":"publication","summary":"In NeurIPS 2015 Workshop on Bayesian Nonparametrics","tags":["workshop"],"title":"Edge-exchangeable graphs, sparsity, and power laws","type":"publication"},{"authors":["admin","Nathanael Ackerman","Cameron Freer"],"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"e7e646a2137065633c6f813ad4c06d02","permalink":"https://dicai.github.io/publication/conference-paper/2014-isfe/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/2014-isfe/","section":"publication","summary":"arXiv e-print 1412.2129","tags":["preprint"],"title":"An iterative step-function estimator for graphons","type":"publication"},{"authors":["admin"],"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"b1159f608ebecb41b9c054705c61e309","permalink":"https://dicai.github.io/publication/theses/2014-thesis/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/publication/theses/2014-thesis/","section":"publication","summary":"","tags":["thesis"],"title":"Scalable methods for Bayesian online changepoint detection","type":"publication"}]