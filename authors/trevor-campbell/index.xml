<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Trevor Campbell on Diana Cai</title>
    <link>https://dicai.github.io/authors/trevor-campbell/</link>
    <description>Recent content in Trevor Campbell on Diana Cai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://dicai.github.io/authors/trevor-campbell/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Finite mixture models do not reliably learn the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2021-finite-mixtures-unreliable/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2021-finite-mixtures-unreliable/</guid>
      <description>Preliminary versions appeared in NeurIPS 2019 Workshop on Machine Learning with Guarantees and also the Symposium on Advances in Approximate Bayesian Inference 2017, co-located with NeurIPS 2017.
Citation @InProceedings{pmlr-v139-cai21a, title = {Finite mixture models do not reliably learn the number of components}, author = {Cai, Diana and Campbell, Trevor and Broderick, Tamara}, booktitle = {Proceedings of the 38th International Conference on Machine Learning}, pages = {1158--1169}, year = {2021}, volume = {139}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR} } </description>
    </item>
    
    <item>
      <title>Power posteriors do not reliably learn the number of components in a finite mixture</title>
      <link>https://dicai.github.io/publication/conference-paper/2020-power-mix/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2020-power-mix/</guid>
      <description>See also: Finite mixture models do not reliably learn the number of components (arXiv e-print 2007.04470) for results on learning the number of components in a finite mixture model via the usual posterior distribution.</description>
    </item>
    
    <item>
      <title>Finite mixture models are typically inconsistent for the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2019-finmix/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2019-finmix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exchangeable trait allocations</title>
      <link>https://dicai.github.io/publication/journal-article/2018-traitexch/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/journal-article/2018-traitexch/</guid>
      <description>Trait allocations are a class of combinatorial structures in which data may belong to multiple groups and may have different levels of belonging in each group. Often the data are also exchangeable, i.e., their joint distribution is invariant to reordering. In clustering—a special case of trait allocation—exchangeability &amp;hellip;</description>
    </item>
    
    <item>
      <title>Finite mixture models are typically inconsistent for the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2017-finmix/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2017-finmix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge-exchangeable graphs and sparsity</title>
      <link>https://dicai.github.io/publication/conference-paper/2016-edgeexch/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2016-edgeexch/</guid>
      <description>Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse &amp;hellip;</description>
    </item>
    
    <item>
      <title>Paintboxes and probability functions for edge-exchangeable graphs</title>
      <link>https://dicai.github.io/publication/conference-paper/2016-paintbox/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2016-paintbox/</guid>
      <description>In NeurIPS 2016 Workshop on Adaptive and Scalable Nonparametric Methods in Machine Learning</description>
    </item>
    
  </channel>
</rss>
