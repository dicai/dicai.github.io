<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on Diana Cai</title>
    <link>https://dicai.github.io/publication/</link>
    <description>Recent content in Publications on Diana Cai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://dicai.github.io/publication/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kernel density Bayesian inverse reinforcement learning</title>
      <link>https://dicai.github.io/publication/conference-paper/2022-bayesian-irl-kernel-density/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2022-bayesian-irl-kernel-density/</guid>
      <description>A preliminary version appeared in the Symposium on Advances in Approximate Bayesian Inference, 2022.</description>
    </item>
    
    <item>
      <title>Optimizing the design of spatial genomics studies</title>
      <link>https://dicai.github.io/publication/journal-article/2022-spatial-genomics-experimental-design/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/journal-article/2022-spatial-genomics-experimental-design/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Multi-fidelity Monte Carlo: a pseudo-marginal approach</title>
      <link>https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-mcmc/</link>
      <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-mcmc/</guid>
      <description>Citation @inproceedings{cai2022multi, title={Multi-fidelity Monte Carlo: a pseudo-marginal approach}, author={Diana Cai and Ryan P Adams}, booktitle={Advances in Neural Information Processing Systems 35}, year={2022} } </description>
    </item>
    
    <item>
      <title>Multi-fidelity Bayesian experimental design using power posteriors</title>
      <link>https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-experimental-design/</link>
      <pubDate>Wed, 02 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2022-multi-fidelity-experimental-design/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Efficient Bayesian inverse reinforcement learning via conditional kernel density estimation</title>
      <link>https://dicai.github.io/publication/conference-paper/2022-birl-aabi/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2022-birl-aabi/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Slice sampling reparameterization gradients</title>
      <link>https://dicai.github.io/publication/conference-paper/2021-reparameterized-gradients-slice-sampling/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2021-reparameterized-gradients-slice-sampling/</guid>
      <description>Preliminary version appeared in the AABI Symposium in January 2021.
Citation @inproceedings{zoltowski2021slice, title={Slice Sampling Reparameterization Gradients}, author={David M. Zoltowski and Diana Cai and Ryan P Adams}, booktitle={Advances in Neural Information Processing Systems 34}, year={2021} } </description>
    </item>
    
    <item>
      <title>Finite mixture models do not reliably learn the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2021-finite-mixtures-unreliable/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2021-finite-mixtures-unreliable/</guid>
      <description>Preliminary versions appeared in NeurIPS 2019 Workshop on Machine Learning with Guarantees and also the Symposium on Advances in Approximate Bayesian Inference 2017, co-located with NeurIPS 2017.
Citation @InProceedings{pmlr-v139-cai21a, title = {Finite mixture models do not reliably learn the number of components}, author = {Cai, Diana and Campbell, Trevor and Broderick, Tamara}, booktitle = {Proceedings of the 38th International Conference on Machine Learning}, pages = {1158--1169}, year = {2021}, volume = {139}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR} } </description>
    </item>
    
    <item>
      <title>Active multi-fidelity Bayesian online changepoint detection</title>
      <link>https://dicai.github.io/publication/conference-paper/2021-active-multi-fidelity-bayesian-changepoint-detection/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2021-active-multi-fidelity-bayesian-changepoint-detection/</guid>
      <description>Citation @article{gundersen2021active, title={Active multi-fidelity Bayesian online changepoint detection}, author={Gundersen, Gregory W and Cai, Diana and Zhou, Chuteng and Engelhardt, Barbara E and Adams, Ryan P}, journal={arXiv preprint arXiv:2103.14224}, year={2021} } </description>
    </item>
    
    <item>
      <title>Slice sampling reparameterization gradients</title>
      <link>https://dicai.github.io/publication/conference-paper/2021-reparam-slice-wkshp/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2021-reparam-slice-wkshp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Power posteriors do not reliably learn the number of components in a finite mixture</title>
      <link>https://dicai.github.io/publication/conference-paper/2020-power-mix/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2020-power-mix/</guid>
      <description>See also: Finite mixture models do not reliably learn the number of components (arXiv e-print 2007.04470) for results on learning the number of components in a finite mixture model via the usual posterior distribution.</description>
    </item>
    
    <item>
      <title>Weighted meta-learning</title>
      <link>https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning-wkshp/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning-wkshp/</guid>
      <description>A longer version of this article is available on the arXiv (link).</description>
    </item>
    
    <item>
      <title>Weighted meta-learning</title>
      <link>https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2020-weighted-meta-learning/</guid>
      <description>Preliminary version appeared in the ICML 2020 Workshop on AutoML.
Citation @article{cai2020weighted, title={Weighted Meta-Learning}, author={Cai, Diana and Sheth, Rishit and Mackey, Lester and Fusi, Nicolo}, journal={arXiv preprint arXiv:2003.09465}, year={2020} } </description>
    </item>
    
    <item>
      <title>Finite mixture models are typically inconsistent for the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2019-finmix/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2019-finmix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Bayesian nonparametric view on count-min sketch</title>
      <link>https://dicai.github.io/publication/conference-paper/2018-cmsketch/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2018-cmsketch/</guid>
      <description>Citation @inproceedings{cai2018bayesian, title={A Bayesian nonparametric view on count-min sketch}, author={Cai, Diana and Mitzenmacher, Michael and Adams, Ryan P}, booktitle={Advances in Neural Information Processing Systems 31}, pages={8782--8791}, year={2018} } </description>
    </item>
    
    <item>
      <title>Exchangeable trait allocations</title>
      <link>https://dicai.github.io/publication/journal-article/2018-traitexch/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/journal-article/2018-traitexch/</guid>
      <description>Trait allocations are a class of combinatorial structures in which data may belong to multiple groups and may have different levels of belonging in each group. Often the data are also exchangeable, i.e., their joint distribution is invariant to reordering. In clustering—a special case of trait allocation—exchangeability &amp;hellip;</description>
    </item>
    
    <item>
      <title>Finite mixture models are typically inconsistent for the number of components</title>
      <link>https://dicai.github.io/publication/conference-paper/2017-finmix/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2017-finmix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge-exchangeable graphs and sparsity</title>
      <link>https://dicai.github.io/publication/conference-paper/2016-edgeexch/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2016-edgeexch/</guid>
      <description>Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse &amp;hellip;</description>
    </item>
    
    <item>
      <title>Paintboxes and probability functions for edge-exchangeable graphs</title>
      <link>https://dicai.github.io/publication/conference-paper/2016-paintbox/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2016-paintbox/</guid>
      <description>In NeurIPS 2016 Workshop on Adaptive and Scalable Nonparametric Methods in Machine Learning</description>
    </item>
    
    <item>
      <title>Priors on exchangeable directed graphs</title>
      <link>https://dicai.github.io/publication/journal-article/2016-digraphons/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/journal-article/2016-digraphons/</guid>
      <description>Directed graphs occur throughout statistical modeling of networks, and exchangeability is a natural assumption when the ordering of vertices does not matter. There is a deep structural theory for exchangeable undirected graphs, which extends to the directed case via measurable objects known as digraphons &amp;hellip;</description>
    </item>
    
    <item>
      <title>Completely random measures for modeling power laws in graphs</title>
      <link>https://dicai.github.io/publication/conference-paper/2015-powerlaws/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2015-powerlaws/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge-exchangeable graphs and sparsity</title>
      <link>https://dicai.github.io/publication/conference-paper/2015-edgeexch-networks/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2015-edgeexch-networks/</guid>
      <description>In NeurIPS 2015 Workshop on Networks in the Social and Information Sciences</description>
    </item>
    
    <item>
      <title>Edge-exchangeable graphs, sparsity, and power laws</title>
      <link>https://dicai.github.io/publication/conference-paper/2015-edgeexch-bnp/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2015-edgeexch-bnp/</guid>
      <description>In NeurIPS 2015 Workshop on Bayesian Nonparametrics</description>
    </item>
    
    <item>
      <title>An iterative step-function estimator for graphons</title>
      <link>https://dicai.github.io/publication/conference-paper/2014-isfe/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/conference-paper/2014-isfe/</guid>
      <description>arXiv e-print 1412.2129</description>
    </item>
    
    <item>
      <title>Scalable methods for Bayesian online changepoint detection</title>
      <link>https://dicai.github.io/publication/theses/2014-thesis/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://dicai.github.io/publication/theses/2014-thesis/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
